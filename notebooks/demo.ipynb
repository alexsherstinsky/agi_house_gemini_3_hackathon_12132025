{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Healing Time Parser Demo\n",
    "\n",
    "This notebook demonstrates the self-healing time parser system that uses an LLM-based coding agent to automatically update parsing logic when encountering new time expression patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root: the directory that contains notebooks/ as a subdirectory\n",
    "cwd = Path.cwd()\n",
    "\n",
    "# Start from current directory and walk up until we find a directory with notebooks/ subdirectory\n",
    "project_root = None\n",
    "current = cwd\n",
    "while current != current.parent:  # Stop at filesystem root\n",
    "    if (current / \"notebooks\").exists() and (current / \"notebooks\").is_dir():\n",
    "        project_root = current\n",
    "        break\n",
    "    current = current.parent\n",
    "\n",
    "# If not found, fallback to current directory\n",
    "if project_root is None:\n",
    "    project_root = cwd\n",
    "    print(f\"⚠ Warning: Could not find project root (directory with notebooks/ subdirectory)\")\n",
    "    print(f\"   Using current directory as fallback: {project_root}\")\n",
    "\n",
    "# Add project root to path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Current working directory: {cwd}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python path includes project root: {str(project_root) in sys.path}\")\n",
    "\n",
    "# Verify coding_agent can be found\n",
    "try:\n",
    "    import coding_agent\n",
    "    print(f\"✓ coding_agent module found at: {coding_agent.__file__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ coding_agent module not found: {e}\")\n",
    "    print(f\"  Please ensure you're using the correct Python environment where the package is installed.\")\n",
    "    print(f\"  Try: uv run jupyter notebook notebooks/demo.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from coding_agent.agent import CodingAgentWorkflow\n",
    "from coding_agent.base import (\n",
    "    AnnotationState,\n",
    "    DEFAULT_RATE_LIMITING_CONFIG,\n",
    ")\n",
    "from coding_agent.error_queue import get_error_count, read_error_queue\n",
    "from coding_agent.llms import NodeLLMs\n",
    "from coding_agent.prompts import build_node_prompts\n",
    "from coding_agent.reloader import reload_parser\n",
    "from coding_agent.test_runner import run_pytest\n",
    "from time_parser import TimeParser\n",
    "from time_parser.wrapper import intercept_parser_errors\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "\n",
    "# Configure pandas display options\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Show full width of each column\n",
    "pd.set_option('display.width', None)  # Auto-detect terminal width\n",
    "pd.set_option('display.max_rows', 100)  # Show up to 100 rows (adjust as needed)\n",
    "\n",
    "print(\"✓ Pandas display options configured for full column width\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "print(\"✓ Logging configured\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini LLM\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable not set\")\n",
    "\n",
    "# Try gemini-3\n",
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-3-pro-preview\",\n",
    "        google_api_key=api_key,\n",
    "        temperature=0.2,\n",
    "        max_output_tokens=8192,\n",
    "    )\n",
    "    print(\"✓ Using gemini-3\")\n",
    "    print(f\"✓ LLM initialized: gemini-3\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to initialize gemini-3: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Demonstration\n",
    "\n",
    "Let's first examine the problem we're trying to solve - some timing descriptions fail to parse while others succeed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fixture file (use project_root from cell 1)\n",
    "fixture_path = project_root / \"tests\" / \"fixtures\" / \"follow_up_tasks_202512121435.jsonl\"\n",
    "if not fixture_path.exists():\n",
    "    raise FileNotFoundError(f\"Fixture file not found: {fixture_path}\")\n",
    "df = pd.read_json(str(fixture_path), lines=True)\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show rows where parsing failed (deadline_at is null)\n",
    "failed_parses = df[df['deadline_at'].isna()][['customer_id', 'timing_description', 'deadline_at']]\n",
    "print(f\"Rows with parsing failures: {len(failed_parses)}\")\n",
    "print(\"\\nSample parsing failures:\")\n",
    "failed_parses.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show rows where parsing succeeded (deadline_at has value)\n",
    "successful_parses = df[df['deadline_at'].notna()][['customer_id', 'timing_description', 'deadline_at']]\n",
    "print(f\"Rows with successful parses: {len(successful_parses)}\")\n",
    "print(\"\\nSample successful parses:\")\n",
    "successful_parses.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear parsers directory (start with empty state)\n",
    "parsers_dir = Path(\"time_parser/parsers\")\n",
    "if parsers_dir.exists():\n",
    "    # Remove all Python files except __init__.py\n",
    "    for parser_file in parsers_dir.glob(\"*.py\"):\n",
    "        if parser_file.name != \"__init__.py\":\n",
    "            parser_file.unlink()\n",
    "            print(f\"✓ Removed {parser_file.name}\")\n",
    "    print(f\"✓ Cleared parsers directory: {parsers_dir}\")\n",
    "else:\n",
    "    print(f\"✓ Parsers directory does not exist yet: {parsers_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show parsers directory (initially empty)\n",
    "from pathlib import Path\n",
    "\n",
    "parsers_dir = Path(\"time_parser/parsers\")\n",
    "print(f\"Parsers directory: {parsers_dir}\")\n",
    "print(f\"Exists: {parsers_dir.exists()}\")\n",
    "\n",
    "if parsers_dir.exists():\n",
    "    parser_files = list(parsers_dir.glob(\"*.py\"))\n",
    "    print(f\"Parser modules: {[f.name for f in parser_files if f.name != '__init__.py']}\")\n",
    "else:\n",
    "    print(\"Parsers directory does not exist yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test initial parser\n",
    "parser = TimeParser()\n",
    "\n",
    "# Test with basic inputs\n",
    "test_inputs = [\"asap\", \"now\", \"tomorrow\"]\n",
    "\n",
    "print(\"Testing initial parser:\")\n",
    "for input_text in test_inputs:\n",
    "    try:\n",
    "        result = parser.parse(input_text)\n",
    "        print(f\"✓ Parsed '{input_text}': {result}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"✗ Failed to parse '{input_text}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Collection\n",
    "\n",
    "Now let's wrap the parser with the exception interceptor to automatically log parsing failures to the error queue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing error queue\n",
    "error_queue_path = Path(\"error_queue.jsonl\")\n",
    "if error_queue_path.exists():\n",
    "    error_queue_path.unlink()\n",
    "    print(\"✓ Cleared existing error queue\")\n",
    "\n",
    "# Wrap parser with exception interceptor\n",
    "wrapped_parse = intercept_parser_errors(\n",
    "    parser,\n",
    "    queue_path=str(error_queue_path),\n",
    ")(parser.parse)\n",
    "\n",
    "print(\"✓ Parser wrapped with exception interceptor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parser with various inputs that will fail\n",
    "test_inputs = [\n",
    "    \"tomorrow\",\n",
    "    \"next week\",\n",
    "    \"in 2 days\",\n",
    "    \"Monday morning\",\n",
    "    \"By 9 AM on Monday\",\n",
    "    \"Within 1-2 business days\",\n",
    "    \"After the initial service appointment is completed\",\n",
    "]\n",
    "\n",
    "print(\"Testing parser with various inputs (errors will be logged):\")\n",
    "for input_text in test_inputs:\n",
    "    try:\n",
    "        result = wrapped_parse(input_text)\n",
    "        print(f\"✓ Parsed '{input_text}': {result}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"✗ Failed to parse '{input_text}' (logged to error queue)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display error queue contents\n",
    "errors = read_error_queue(error_queue_path)\n",
    "print(f\"Total errors in queue: {len(errors)}\")\n",
    "print(\"\\nSample errors:\")\n",
    "for i, error in enumerate(errors[:5]):\n",
    "    print(f\"{i+1}. {error.get('timing_description', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview: The REASON node will cluster these errors by semantic similarity. For example:**\n",
    "- **Relative dates cluster**: \"tomorrow\", \"next week\", \"in 2 days\", \"Monday morning\"\n",
    "- **Specific dates with times cluster**: \"By 9 AM on Monday\"\n",
    "- **Time ranges cluster**: \"Within 1-2 business days\"\n",
    "- **Context-dependent cluster**: \"After the initial service appointment is completed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Activation\n",
    "\n",
    "Now let's activate the coding agent to analyze errors, generate parser modules, and update the parser automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check error count threshold\n",
    "error_count = get_error_count(error_queue_path)\n",
    "print(f\"Errors in queue: {error_count}\")\n",
    "\n",
    "from coding_agent.config import ERROR_THRESHOLD\n",
    "print(f\"Error threshold: {ERROR_THRESHOLD}\")\n",
    "\n",
    "if error_count >= ERROR_THRESHOLD:\n",
    "    print(\"✓ Enough errors to activate agent\")\n",
    "else:\n",
    "    print(f\"⚠ Need at least {ERROR_THRESHOLD} errors to activate agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build NodePrompts\n",
    "node_prompts = build_node_prompts()\n",
    "print(\"✓ NodePrompts created (template user prompts, not formatted)\")\n",
    "\n",
    "# Build NodeLLMs\n",
    "node_llms = NodeLLMs(reason=llm, plan=llm, act=llm, validate=llm)\n",
    "print(\"✓ NodeLLMs configured\")\n",
    "\n",
    "# Generate thread_id\n",
    "thread_id = f\"coding_agent_{uuid.uuid4().hex[:8]}\"\n",
    "print(f\"✓ Thread ID: {thread_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CodingAgentWorkflow\n",
    "workflow = CodingAgentWorkflow(\n",
    "    node_llms=node_llms,\n",
    "    node_prompts=node_prompts,  # Template prompts, not formatted\n",
    "    thread_id=thread_id,\n",
    "    error_queue_path=\"error_queue.jsonl\",\n",
    "    parsers_dir=\"time_parser/parsers\",\n",
    "    tests_dir=\"time_parser/tests\",\n",
    "    rate_limiting_config=DEFAULT_RATE_LIMITING_CONFIG,\n",
    "    fail_fast=False,\n",
    "    error_logging=True,\n",
    "    debug_logging=False,\n",
    "    enforce_structured_llm_output=False,  # Must be False for schema=None\n",
    ")\n",
    "\n",
    "print(\"✓ CodingAgentWorkflow initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial state\n",
    "initial_state = AnnotationState(\n",
    "    messages=[],\n",
    "    node_output=None,\n",
    "    final_output=None,\n",
    ")\n",
    "\n",
    "print(\"✓ Initial state created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agent workflow\n",
    "print(\"Running coding agent workflow...\")\n",
    "print(\"This will:\")\n",
    "print(\"  1. REASON: Cluster errors by semantic similarity\")\n",
    "print(\"  2. PLAN: Design code changes and test strategy\")\n",
    "print(\"  3. ACT: Generate parser modules and test files\")\n",
    "print(\"  4. VALIDATE: Run tests and verify all pass\")\n",
    "print()\n",
    "\n",
    "result = workflow.run(initial_state=initial_state)\n",
    "\n",
    "print(\"\\n✓ Workflow completed!\")\n",
    "print(f\"Result keys: {list(result.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display agent results\n",
    "print(\"Agent Results:\")\n",
    "print(f\"  Success: {result.get('success', False)}\")\n",
    "print(f\"  Processed clusters: {result.get('processed_clusters', [])}\")\n",
    "print(f\"  Errors removed: {result.get('errors_removed_count', 0)}\")\n",
    "print(f\"  Parser updated: {result.get('parser_updated', False)}\")\n",
    "print(f\"  Tests passed: {result.get('tests_passed', False)}\")\n",
    "print(f\"  Retry count: {result.get('retry_count', 0)}\")\n",
    "print(f\"  Generated modules: {list(result.get('generated_cluster_modules', {}).keys())}\")\n",
    "print(f\"  Generated test files: {list(result.get('generated_test_files', {}).keys())}\")\n",
    "\n",
    "if result.get('message'):\n",
    "    print(f\"  Message: {result['message']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Verification\n",
    "\n",
    "Let's verify that the parser now works with previously failing inputs and check the test results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload parser after agent update\n",
    "parser = reload_parser(\"time_parser/parsers\")\n",
    "print(\"✓ Parser reloaded with updated cluster modules\")\n",
    "\n",
    "# Show updated parsers directory\n",
    "parsers_dir = Path(\"time_parser/parsers\")\n",
    "if parsers_dir.exists():\n",
    "    parser_files = list(parsers_dir.glob(\"*.py\"))\n",
    "    print(f\"Parser modules: {[f.name for f in parser_files if f.name != '__init__.py']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parser with previously failing inputs\n",
    "previously_failing = [\n",
    "    \"tomorrow\",\n",
    "    \"next week\",\n",
    "    \"in 2 days\",\n",
    "    \"Monday morning\",\n",
    "]\n",
    "\n",
    "print(\"Testing parser with previously failing inputs:\")\n",
    "for input_text in previously_failing:\n",
    "    try:\n",
    "        result = parser.parse(input_text)\n",
    "        print(f\"✓ Parsed '{input_text}': {result}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"✗ Still failed to parse '{input_text}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pytest and display results\n",
    "test_results = run_pytest(\"time_parser/tests\", verbose=True)\n",
    "\n",
    "print(f\"All tests passed: {test_results['all_passed']}\")\n",
    "print(f\"Return code: {test_results['returncode']}\")\n",
    "print(f\"\\nTest output:\\n{test_results['test_output']}\")\n",
    "\n",
    "if test_results['test_errors']:\n",
    "    print(f\"\\nTest errors:\\n{test_results['test_errors']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show updated error queue (should have fewer errors)\n",
    "remaining_errors = read_error_queue(error_queue_path)\n",
    "print(f\"Remaining errors in queue: {len(remaining_errors)}\")\n",
    "print(f\"Errors removed: {result.get('errors_removed_count', 0)}\")\n",
    "\n",
    "if remaining_errors:\n",
    "    print(\"\\nRemaining errors (not yet processed):\")\n",
    "    for i, error in enumerate(remaining_errors[:5]):\n",
    "        print(f\"  {i+1}. {error.get('timing_description', 'N/A')}\")\n",
    "else:\n",
    "    print(\"\\n✓ All errors have been processed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Production Data Validation\n",
    "\n",
    "Now let's validate the system with real production data from the fixture file. This demonstrates that the self-healing parser works not just with controlled test inputs, but also with actual production failures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear error queue again for production data validation\n",
    "if error_queue_path.exists():\n",
    "    error_queue_path.unlink()\n",
    "    print(\"✓ Cleared error queue for production data validation\")\n",
    "else:\n",
    "    print(\"✓ Error queue already empty\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fixture data and populate error queue with real production failures\n",
    "from coding_agent.error_queue import append_error_to_queue\n",
    "\n",
    "# Use project_root from cell 1 for fixture path\n",
    "fixture_path = project_root / \"tests\" / \"fixtures\" / \"follow_up_tasks_202512121435.jsonl\"\n",
    "if not fixture_path.exists():\n",
    "    raise FileNotFoundError(f\"Fixture file not found: {fixture_path}\")\n",
    "df_production = pd.read_json(str(fixture_path), lines=True)\n",
    "\n",
    "# Filter for rows where deadline_at is null (parsing failures)\n",
    "failed_parses_production = df_production[df_production['deadline_at'].isna()]\n",
    "\n",
    "print(f\"Total rows in fixture: {len(df_production)}\")\n",
    "print(f\"Rows with parsing failures: {len(failed_parses_production)}\")\n",
    "\n",
    "# Add production errors to error queue\n",
    "errors_added = 0\n",
    "for _, row in failed_parses_production.iterrows():\n",
    "    error_entry = {\n",
    "        \"customer_id\": row.get('customer_id'),\n",
    "        \"deadline_at\": None,\n",
    "        \"timing_description\": row.get('timing_description'),\n",
    "        \"auxiliary_pretty\": row.get('auxiliary_pretty', '{}'),\n",
    "    }\n",
    "    \n",
    "    # Validate timing_description is a non-empty string\n",
    "    timing_desc = error_entry.get('timing_description', '')\n",
    "    if isinstance(timing_desc, str) and timing_desc.strip():\n",
    "        append_error_to_queue(str(error_queue_path), error_entry)\n",
    "        errors_added += 1\n",
    "\n",
    "print(f\"✓ Added {errors_added} production errors to error queue\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample production errors in queue\n",
    "production_errors = read_error_queue(error_queue_path)\n",
    "print(f\"Total production errors in queue: {len(production_errors)}\")\n",
    "print(\"\\nSample production errors:\")\n",
    "for i, error in enumerate(production_errors[:10]):\n",
    "    timing_desc = error.get('timing_description', 'N/A')\n",
    "    print(f\"  {i+1}. {timing_desc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have enough errors to run the agent\n",
    "production_error_count = get_error_count(error_queue_path)\n",
    "print(f\"Production errors in queue: {production_error_count}\")\n",
    "print(f\"Error threshold: {ERROR_THRESHOLD}\")\n",
    "\n",
    "if production_error_count >= ERROR_THRESHOLD:\n",
    "    print(\"✓ Enough production errors to activate agent\")\n",
    "    print(\"\\nNote: You can run the agent workflow again to process these production errors.\")\n",
    "    print(\"The workflow would cluster these real production patterns and generate\")\n",
    "    print(\"parser modules to handle them, just as it did with the controlled test inputs.\")\n",
    "else:\n",
    "    print(f\"⚠ Need at least {ERROR_THRESHOLD} errors to activate agent\")\n",
    "    print(f\"   (Current: {production_error_count})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional: Run Agent on Production Data**\n",
    "\n",
    "The cell below can be executed to process the production errors through the agent workflow. This demonstrates the full cycle with real production data. (You can skip this if you've already demonstrated the workflow above.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Run agent workflow on production errors\n",
    "# Uncomment the code below to process production errors through the agent\n",
    "\n",
    "# if production_error_count >= ERROR_THRESHOLD:\n",
    "#     # Generate new thread_id for this run\n",
    "#     production_thread_id = f\"coding_agent_production_{uuid.uuid4().hex[:8]}\"\n",
    "#     \n",
    "#     # Initialize workflow (reuse same configuration)\n",
    "#     production_workflow = CodingAgentWorkflow(\n",
    "#         node_llms=node_llms,\n",
    "#         node_prompts=node_prompts,\n",
    "#         thread_id=production_thread_id,\n",
    "#         error_queue_path=\"error_queue.jsonl\",\n",
    "#         parsers_dir=\"time_parser/parsers\",\n",
    "#         tests_dir=\"time_parser/tests\",\n",
    "#         rate_limiting_config=DEFAULT_RATE_LIMITING_CONFIG,\n",
    "#         fail_fast=False,\n",
    "#         error_logging=True,\n",
    "#         debug_logging=False,\n",
    "#         enforce_structured_llm_output=False,\n",
    "#     )\n",
    "#     \n",
    "#     # Create initial state\n",
    "#     production_initial_state = AnnotationState(\n",
    "#         messages=[],\n",
    "#         node_output=None,\n",
    "#         final_output=None,\n",
    "#     )\n",
    "#     \n",
    "#     # Run workflow\n",
    "#     print(\"Running agent workflow on production errors...\")\n",
    "#     production_result = production_workflow.run(initial_state=production_initial_state)\n",
    "#     \n",
    "#     print(\"\\n✓ Production workflow completed!\")\n",
    "#     print(f\"Success: {production_result.get('success', False)}\")\n",
    "#     print(f\"Processed clusters: {production_result.get('processed_clusters', [])}\")\n",
    "#     print(f\"Errors removed: {production_result.get('errors_removed_count', 0)}\")\n",
    "#     \n",
    "#     # Reload parser and test with production patterns\n",
    "#     parser = reload_parser(\"time_parser/parsers\")\n",
    "#     print(\"\\n✓ Parser reloaded with production-generated modules\")\n",
    "# else:\n",
    "#     print(\"Not enough errors to run agent workflow\")\n",
    "\n",
    "print(\"(Cell is commented out - uncomment to run agent on production data)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The self-healing time parser system has successfully:\n",
    "1. ✅ Collected parsing failures in the error queue\n",
    "2. ✅ Clustered errors by semantic similarity\n",
    "3. ✅ Generated parser modules for each error cluster\n",
    "4. ✅ Created comprehensive test files\n",
    "5. ✅ Validated all tests pass\n",
    "6. ✅ Updated the parser with new capabilities\n",
    "7. ✅ Removed processed errors from the queue\n",
    "\n",
    "The parser can now handle previously failing time expressions automatically!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
