# Development Plan Sprint 1: Self-Healing Time Parser - Foundation and LangGraph Integration

## Overview

This sprint implements a self-healing time parser system that uses an LLM-based coding agent to automatically update parsing logic when encountering new time expression patterns. The system learns from failures and improves itself without manual intervention. This sprint establishes the foundation (MVP) and integrates LangGraph workflow with REASON/PLAN/ACT/VALIDATE pattern. The implementation uses Google Gemini 3 via LangChain, implements modular parser architecture with cluster-specific modules, and includes comprehensive test-driven validation.

## Critical Directives

1. **Confirm before marking tasks** - Never mark a task as complete with checkmark (✓) without explicit user permission.
2. **Ask permission to proceed** - After marking a task complete, state the next task and request permission before continuing.
3. **Never proceed without permission** - Always wait for explicit approval before starting any task.
4. **Follow specification exactly** - Adhere strictly to this design specification and referenced design documents.
5. **Follow coding standards** - Obey all Python coding standards (PEP 8, type hints, docstrings).
6. **Pure ASCII markdown** - Keep all sprint documentation in ASCII-only markdown format.
7. **Test after each step** - Run tests and manual verification for affected components after implementing each functional chunk.
8. **Preserve existing functionality** - Ensure no regressions in existing codebase utilities.
9. **Modular architecture** - Use modular architecture with one module per error cluster (not single large file).
10. **UTC timezone requirement** - All datetime objects must be timezone-aware with UTC timezone.

## Sprint Goals

- Install project dependencies via pyproject.toml.
- Create TimeParser class with dynamic cluster module loading.
- Implement exception interceptor/wrapper for error logging.
- Create error queue management (JSONL format with append/read/remove functionality).
- Implement prompt templates for REASON, PLAN, and ACT nodes.
- Create CodingAgentWorkflow class extending WorkflowBase with LangGraph R/P/A pattern.
- Implement REASON node for error clustering using LLM.
- Implement PLAN node for code planning using LLM.
- Implement ACT node for code generation using LLM.
- Implement VALIDATE node for pytest test execution.
- Add retry logic with conditional edges and safety valve (failed batch logging).
- Create demo Jupyter notebook with complete self-healing flow demonstration.
- Test complete end-to-end flow (error collection → clustering → code generation → validation → parser update).

## Implementation Clarifications

### Module Paths and Locations

1. **Time Parser Module:**
   - Main parser: `time_parser/parser.py` (orchestrator, rarely updated)
   - Cluster modules: `time_parser/parsers/<cluster_id>.py` (generated by agent, one per cluster)
   - Exception wrapper: `time_parser/wrapper.py` (error interceptor)

2. **Test Directory:**
   - Test files: `time_parser/tests/test_<cluster_id>.py` (one per cluster module)
   - Shared utilities: `time_parser/tests/test_utils.py`
   - Shared fixtures: `time_parser/tests/conftest.py`

3. **Coding Agent:**
   - Main workflow: `coding_agent/agent.py` (CodingAgentWorkflow class)
   - Configuration: `coding_agent/config.py` (constants: CLUSTER_BATCH_SIZE, MAX_RETRY_ATTEMPTS, ERROR_THRESHOLD)
   - Queue management: `coding_agent/error_queue.py` (read, append, remove functions)
   - Test runner: `coding_agent/test_runner.py` (pytest integration)
   - Module reloader: `coding_agent/reloader.py` (dynamic module reloading)
   - Prompt templates: `coding_agent/prompts.py` (add prompt template strings)

4. **Utilities:**
   - LLM helpers: `utils/llm_helpers.py` (call_llm_with_prompt function - already exists)
   - Dynamic loading: `utils/dynamic_loading.py` (module loading utilities - already exists)

5. **Demo Notebook:**
   - File: `notebooks/demo.ipynb` (Jupyter notebook for demonstration)

6. **Runtime Files:**
   - Error queue: `error_queue.jsonl` (repository root, gitignored)
   - Failed batches: `failed_batches.jsonl` (repository root, gitignored)

7. **Test Fixtures:**
   - Error examples: `tests/fixtures/follow_up_tasks_202512121435.jsonl` (source data for testing and demonstration)

### Architectural Decisions

1. **Modular Parser Architecture:**
   - Main parser (`time_parser/parser.py`) orchestrates by discovering and loading cluster-specific modules
   - Each error cluster gets its own module file in `time_parser/parsers/`
   - Each module exports a `parse(text: str) -> datetime | None` function
   - Main parser tries each cluster module in sequence until one succeeds
   - No limit on number of clusters - scales naturally with new module files

2. **LLM Integration:**
   - Use Google Gemini 3 via LangChain (`langchain_google_genai.ChatGoogleGenerativeAI`)
   - API key from `GOOGLE_API_KEY` environment variable
   - Use `call_llm_with_prompt()` from `utils/llm_helpers.py` with `schema=None` (schema-less output)
   - Parse JSON responses manually from LLM text output
   - Temperature: 0.1-0.3 for code generation
   - Model: "gemini-3" (or "gemini-2.5-flash" as fallback)

3. **LangGraph Workflow Pattern:**
   - Use `WorkflowBase` from `coding_agent/base.py` as base class
   - Extend `AnnotationState` for workflow state management
   - Nodes: REASON → PLAN → ACT → VALIDATE
   - Conditional edge from VALIDATE: success → END, retry → PLAN, failure → END (safety valve)
   - State contains: error_queue_path, error_clusters, selected_clusters, generated_cluster_modules, generated_test_files, test_results, retry_count

4. **Error Queue Format:**
   - JSONL format (one JSON object per line)
   - Fields: `timing_description` (primary), `auxiliary_pretty` (optional), `customer_id`, `deadline_at`
   - Easy to append, read batch, and remove processed items

5. **Error Clustering:**
   - LLM-based semantic clustering in REASON node
   - Process up to CLUSTER_BATCH_SIZE clusters per batch (default: 3, configurable)
   - Prioritize parsable clusters over context-dependent or ambiguous ones
   - Cluster IDs become module filenames (lowercase_with_underscores)

6. **Code Generation:**
   - ACT node generates complete module files (not snippets)
   - One module file per cluster: `time_parser/parsers/<cluster_id>.py`
   - One test file per cluster: `time_parser/tests/test_<cluster_id>.py`
   - All code must be syntactically correct and ready to execute

7. **Test Validation:**
   - VALIDATE node runs pytest on `time_parser/tests/` directory
   - All tests must pass before proceeding to module reload
   - If tests fail, loop back to PLAN node (up to MAX_RETRY_ATTEMPTS)
   - After max retries, log failed batch to `failed_batches.jsonl` and give up

8. **Module Reloading:**
   - After successful validation, reload cluster modules using `importlib.reload()`
   - Call `TimeParser.reload_cluster_modules()` to refresh parser registry
   - Use file locking (fcntl) to prevent race conditions during updates

9. **Exception Interceptor:**
   - Decorator/context manager pattern that wraps parser calls
   - Catches exceptions, logs to error queue, re-raises or returns None
   - Does not block original code execution

10. **Safety Valve:**
    - Failed batches (after max retries) logged to `failed_batches.jsonl`
    - Agent continues with next batch (does not block processing)
    - Failed batches can be reviewed manually later

## Sprint Tasks

### 1.0 Install Dependencies and Verify Environment [S]

   1.1 **Install project dependencies** [S]
       - Navigate to project root: `cd agi_house_gemini_3_hackathon_12132025`
       - Run: `pip install -e .` (installs main dependencies including pandas)
       - Optional: Run `pip install -e ".[dev]"` to also install dev dependencies (jupyter, matplotlib, tqdm)
       - Verify all dependencies install correctly (langchain, langchain-google-genai, langgraph, pydantic, pytest, python-dateutil, pandas)
       - Note: This installs from pyproject.toml

   1.2 **Verify GOOGLE_API_KEY environment variable** [S]
       - Check: `GOOGLE_API_KEY` is set in environment
       - Test: Create simple test script to verify Gemini API access
       - Note: API key should be available before proceeding with LLM integration tasks

### 2.0 Create TimeParser Class with Dynamic Module Loading [M]

   2.1 **Create TimeParser class** [M]
       - File: `time_parser/parser.py`
       - Implement `__init__()`: Initialize `_cluster_parsers` dict and call `_load_cluster_modules()`
       - Implement `_load_cluster_modules()`: Discover all `.py` files in `parsers/` directory, import each module, register `parse()` function
       - Implement `reload_cluster_modules()`: Reload existing modules using `importlib.reload()`, then call `_load_cluster_modules()`
       - Implement `parse(text: str) -> datetime`: Try each cluster parser in order, return first successful result or raise ValueError
       - Handle errors gracefully: If a cluster module fails to load, continue with other modules
       - Test: Verify parser loads modules correctly, handles missing modules gracefully

   2.2 **Create initial basic parser for testing** [S]
       - Initially, parser should handle basic cases: "asap", "now" (for testing)
       - These can be hardcoded in `parse()` method initially
       - Agent will generate cluster modules later
       - Test: Verify basic parsing works before agent integration

### 3.0 Create Exception Interceptor/Wrapper [M]

   3.1 **Create intercept_parser_errors decorator** [M]
       - File: `time_parser/wrapper.py`
       - Implement decorator pattern: `intercept_parser_errors(parser_instance)`
       - Behavior: Catch exceptions from parser, log to error queue (JSONL format), re-raise exception
       - Error entry format: `{"customer_id": None, "deadline_at": None, "timing_description": text, "auxiliary_pretty": json.dumps({...})}`
       - Use `coding_agent.error_queue.append_error_to_queue()` for logging
       - Test: Verify errors are logged correctly to error_queue.jsonl

   3.2 **Test exception interceptor with TimeParser** [S]
       - Create test script that uses wrapped parser
       - Test with inputs that fail parsing
       - Verify errors appear in error_queue.jsonl
       - Verify original code execution continues (does not block)

### 4.0 Implement Prompt Templates [M]

   4.1 **Add REASON node prompt templates** [M]
       - File: `coding_agent/prompts.py`
       - Add: `REASON_NODE_SYSTEM_PROMPT_TEMPLATE` (from Section 18.1 of design doc)
       - Add: `REASON_NODE_USER_PROMPT_TEMPLATE` (from Section 18.1 of design doc)
       - Templates use `{error_queue_contents}` placeholder for formatting
       - Test: Verify templates are valid Python strings, can be formatted

   4.2 **Add PLAN node prompt templates** [M]
       - File: `coding_agent/prompts.py`
       - Add: `PLAN_NODE_SYSTEM_PROMPT_TEMPLATE` (from Section 18.2 of design doc)
       - Add: `PLAN_NODE_USER_PROMPT_TEMPLATE` (from Section 18.2 of design doc)
       - Templates use `{cluster_analysis}` and `{existing_cluster_modules}` placeholders
       - Test: Verify templates are valid Python strings, can be formatted

   4.3 **Add ACT node prompt templates** [M]
       - File: `coding_agent/prompts.py`
       - Add: `ACT_NODE_SYSTEM_PROMPT_TEMPLATE` (from Section 18.3 of design doc)
       - Add: `ACT_NODE_USER_PROMPT_TEMPLATE` (from Section 18.3 of design doc)
       - Templates use `{code_plan}` placeholder for formatting
       - Test: Verify templates are valid Python strings, can be formatted

   4.4 **Create helper function to build NodePrompts** [S]
       - File: `coding_agent/prompts.py`
       - Add function: `build_node_prompts(error_queue_contents: str, cluster_analysis: str, existing_modules: str, code_plan: str) -> NodePrompts`
       - Function formats templates and creates SystemAndUserPromptPair instances
       - Returns NodePrompts with reason, plan, act prompts populated
       - Test: Verify function creates valid NodePrompts object

### 5.0 Create CodingAgentWorkflow Class Structure [M]

   5.1 **Create CodingAgentWorkflow class skeleton** [M]
       - File: `coding_agent/agent.py`
       - Extend `WorkflowBase` from `coding_agent.base`
       - Implement `__init__()`: Accept error_queue_path, parsers_dir, tests_dir, llm configuration, prompts
       - Store configuration as instance variables
       - Import required dependencies: WorkflowBase, AnnotationState, NodeLLMs, NodePrompts, etc.
       - Test: Verify class can be instantiated

   5.2 **Create CodingAgentState class** [S]
       - File: `coding_agent/agent.py`
       - Extend `AnnotationState` from `coding_agent.base`
       - Add custom fields: error_queue_path, parsers_dir, tests_dir, error_clusters, selected_clusters, existing_cluster_modules, generated_cluster_modules, generated_test_files, test_results, retry_count
       - Use Pydantic Field for type hints and defaults
       - Test: Verify state class can be instantiated with initial values

   5.3 **Implement _add_workflow_nodes_and_edges method** [M]
       - File: `coding_agent/agent.py`
       - Add nodes: "reason", "plan", "act", "validate" (call respective node methods)
       - Set entry point: "reason"
       - Add sequential edges: reason → plan → act → validate
       - Add conditional edge from validate: Use `_should_retry()` method
       - Conditional outcomes: "success" → END, "retry" → "plan", "failure" → END
       - Test: Verify workflow graph structure is correct

### 6.0 Implement REASON Node [M]

   6.1 **Implement _reason_node method** [M]
       - File: `coding_agent/agent.py`
       - Read error queue using `coding_agent.error_queue.read_error_queue()`
       - Format error queue contents as JSONL string for prompt
       - Build prompts using `build_node_prompts()` with error_queue_contents
       - Call LLM using `_call_llm_with_prompt()` (from WorkflowBase) with reason node prompts
       - Parse JSON response from LLM (schema=None, so parse from text)
       - Extract clusters and selected_clusters from response
       - Update state with error_clusters and selected_clusters
       - Return updated state
       - Test: Verify REASON node clusters errors correctly with mock LLM response

   6.2 **Add JSON parsing helper for LLM responses** [S]
       - File: `coding_agent/agent.py`
       - Add function: `_parse_json_response(response: AIMessage) -> dict`
       - Handle JSON extraction from text (may be wrapped in markdown code blocks)
       - Handle parsing errors gracefully
       - Test: Verify function parses JSON from various LLM response formats

### 7.0 Implement PLAN Node [M]

   7.1 **Implement _plan_node method** [M]
       - File: `coding_agent/agent.py`
       - Read existing cluster modules from parsers_dir (if any exist)
       - Format cluster_analysis from state (selected clusters from REASON node)
       - Format existing_cluster_modules list (module names and brief descriptions)
       - Build prompts using `build_node_prompts()` with cluster_analysis and existing_modules
       - Call LLM using `_call_llm_with_prompt()` with plan node prompts
       - Parse JSON response (code plan with cluster_plans array)
       - Update state with code_plan
       - Return updated state
       - Test: Verify PLAN node creates code plans correctly with mock LLM response

### 8.0 Implement ACT Node [M]

   8.1 **Implement _act_node method** [M]
       - File: `coding_agent/agent.py`
       - Get code_plan from state
       - Build prompts using `build_node_prompts()` with code_plan
       - Call LLM using `_call_llm_with_prompt()` with act node prompts
       - Parse JSON response (cluster_modules and test_files dictionaries)
       - Write cluster module files to `parsers/<cluster_id>.py`
       - Write test files to `tests/test_<cluster_id>.py`
       - Use file locking (fcntl) when writing files to prevent race conditions
       - Update state with generated_cluster_modules and generated_test_files
       - Return updated state
       - Test: Verify ACT node generates and writes files correctly with mock LLM response

   8.2 **Add file writing helper with locking** [S]
       - File: `coding_agent/agent.py`
       - Add function: `_write_file_with_lock(file_path: Path, content: str) -> None`
       - Use fcntl.flock() for file locking (Unix) or msvcrt for Windows
       - Write to temporary file first, then atomically rename
       - Handle locking errors gracefully
       - Test: Verify file writing works correctly with locking

### 9.0 Implement VALIDATE Node [M]

   9.1 **Implement _validate_node method** [M]
       - File: `coding_agent/agent.py`
       - Use `coding_agent.test_runner.run_pytest()` to run tests
       - Pass tests_dir path to run_pytest
       - Parse test results (all_passed, test_output, returncode)
       - Increment retry_count in state
       - Update state with test_results
       - Return updated state
       - Test: Verify VALIDATE node runs pytest and parses results correctly

   9.2 **Implement _should_retry method** [S]
       - File: `coding_agent/agent.py`
       - Check test_results["all_passed"]: If True, return "success"
       - Check retry_count against MAX_RETRY_ATTEMPTS from config
       - If retry_count < MAX_RETRY_ATTEMPTS, return "retry"
       - Otherwise, call `_log_failed_batch()` and return "failure"
       - Test: Verify retry logic works correctly for all scenarios

   9.3 **Implement _log_failed_batch method** [S]
       - File: `coding_agent/agent.py`
       - Create failed_batch_entry dict with error_batch, test_results, timestamp, retry_count
       - Append to failed_batches.jsonl using JSONL format
       - Use `coding_agent.error_queue.append_error_to_queue()` pattern
       - Test: Verify failed batches are logged correctly

### 10.0 Implement Queue Cleanup and Module Reloading [M]

   10.1 **Add queue cleanup after successful validation** [M]
       - File: `coding_agent/agent.py`
       - After successful validation (all tests pass), collect error indices from processed clusters
       - Use `coding_agent.error_queue.remove_processed_cluster_errors()` to remove errors
       - Map cluster error_indices to global error indices from original queue
       - Update state with errors_removed_count
       - Test: Verify processed errors are removed from queue correctly

   10.2 **Add module reloading after successful validation** [S]
       - File: `coding_agent/agent.py`
       - After successful validation and queue cleanup, call `coding_agent.reloader.reload_parser()`
       - Pass parsers_dir to reloader
       - Verify parser reloads successfully
       - Test: Verify modules are reloaded and parser works with new modules

### 11.0 Create Test Infrastructure [M]

   11.1 **Create test utilities** [S]
       - File: `time_parser/tests/test_utils.py`
       - Add function: `assert_valid_datetime(result: datetime | None, input_text: str) -> None`
       - Assertions: result is not None, is datetime instance, has timezone, is UTC
       - Test: Verify utility function works correctly

   11.2 **Create shared pytest fixtures** [S]
       - File: `time_parser/tests/conftest.py`
       - Add fixture: `parser()` that returns TimeParser instance
       - Test: Verify fixture works in pytest

### 12.0 Create Demo Jupyter Notebook [M]

   12.1 **Create notebook structure** [M]
       - File: `notebooks/demo.ipynb`
       - Follow style of reference_notebook.ipynb (path setup, imports, logging configuration)
       - Add cells for: imports, path setup, logging, Gemini LLM initialization
       - Use GOOGLE_API_KEY from environment
       - Initialize Gemini using langchain_google_genai.ChatGoogleGenerativeAI
       - Model: "gemini-3" (or "gemini-2.5-flash" as fallback)
       - Temperature: 0.2, max_tokens: 8192
       - Test: Verify notebook runs without errors

   12.1a **Add problem demonstration with fixture data** [S]
       - File: `notebooks/demo.ipynb`
       - Add cell: Load fixture file using pandas: `pd.read_json('tests/fixtures/follow_up_tasks_202512121435.jsonl', lines=True)`
       - Add cell: Display sample rows showing the problem:
         - Show a few rows where `deadline_at` is null (parsing failed)
         - Show a few rows where `deadline_at` has a value (parsing succeeded)
       - Use pandas DataFrame display to show: `customer_id`, `timing_description`, `deadline_at` columns
       - Add explanatory text: "This demonstrates the problem - some timing descriptions fail to parse (deadline_at is null), while others succeed"
       - Test: Verify fixture file loads correctly and display shows both error and success cases

   12.2 **Add initial parser state demonstration** [S]
       - File: `notebooks/demo.ipynb`
       - Cell: Show initial TimeParser implementation
       - Cell: Show empty parsers/ directory
       - Cell: Test parser with basic inputs ("asap", "now")
       - Test: Verify cells execute correctly

   12.3 **Add error collection demonstration** [M]
       - File: `notebooks/demo.ipynb`
       - Cell: Wrap parser with exception interceptor
       - Cell: Test parser with various inputs that will fail
       - Cell: Display error queue contents
       - Cell: Show clustering preview (what REASON node will do)
       - Test: Verify errors are collected and displayed

   12.4 **Add agent activation demonstration** [M]
       - File: `notebooks/demo.ipynb`
       - Cell: Check error count threshold
       - Cell: Initialize CodingAgentWorkflow with LLM and prompts
       - Cell: Run agent workflow
       - Cell: Display agent results (processed clusters, errors removed, etc.)
       - Test: Verify agent runs successfully

   12.5 **Add success verification demonstration** [S]
       - File: `notebooks/demo.ipynb`
       - Cell: Reload parser after agent update
       - Cell: Test parser with previously failing inputs
       - Cell: Run pytest and display results
       - Cell: Show updated error queue (should have fewer errors)
       - Test: Verify complete self-healing flow works

### 13.0 Integration Testing and Refinement [M]

   13.1 **Test complete end-to-end flow** [M]
       - Create test error queue with sample errors from `tests/fixtures/follow_up_tasks_202512121435.jsonl`
       - Filter fixture data: Select rows where `deadline_at` is null (these are the parsing failures)
       - Copy selected rows to error_queue.jsonl for testing
       - Run agent workflow
       - Verify: Errors are clustered correctly
       - Verify: Code is generated and written correctly
       - Verify: Tests are generated and pass
       - Verify: Parser is updated and works with new modules
       - Verify: Processed errors are removed from queue
       - Test: Complete flow works without errors

   13.2 **Refine prompts based on actual LLM behavior** [M]
       - Run agent with real errors
       - Observe LLM responses and adjust prompts if needed
       - Ensure JSON output format is consistent
       - Ensure code generation quality is acceptable
       - Test: Prompts produce reliable, high-quality outputs

   13.3 **Add error handling and logging** [S]
       - Add comprehensive error handling throughout workflow
       - Add logging for debugging (use Python logging module)
       - Handle LLM API errors gracefully
       - Handle file I/O errors gracefully
       - Test: Error handling works correctly in various failure scenarios

## Manual Testing Guide

### Prerequisites

- All tasks 1.0-13.0 must be completed
- GOOGLE_API_KEY environment variable set
- Project dependencies installed (`pip install -e .`)
- Jupyter notebook environment available
- Test error data available (from `tests/fixtures/follow_up_tasks_202512121435.jsonl`)

### Test 1: Basic Parser Functionality

**Steps:**

1. **Start Jupyter notebook:**
   ```bash
   cd agi_house_gemini_3_hackathon_12132025
   jupyter notebook notebooks/demo.ipynb
   ```

2. **Run initial setup cells:**
   - Verify path setup works
   - Verify imports succeed
   - Verify Gemini LLM initializes correctly

3. **Test basic parser:**
   - Create TimeParser instance
   - Test with "asap" - should succeed
   - Test with "now" - should succeed
   - Test with "tomorrow" - should fail (not yet implemented)
   - Expected: Basic cases work, unknown patterns raise ValueError

### Test 2: Error Collection

**Steps:**

1. **Wrap parser with exception interceptor:**
   - Use `intercept_parser_errors()` decorator
   - Test with various inputs that will fail

2. **Verify error logging:**
   - Check error_queue.jsonl file
   - Verify errors are logged in correct format
   - Verify timing_description field contains failed input text

3. **Test error queue utilities:**
   - Use `coding_agent.error_queue.read_error_queue()` to read errors
   - Verify error count matches expected

### Test 3: Agent Workflow - REASON Node

**Steps:**

1. **Prepare error queue:**
   - Populate error_queue.jsonl with sample errors (at least ERROR_THRESHOLD errors)
   - Use errors from `tests/fixtures/follow_up_tasks_202512121435.jsonl`
   - Filter for rows where `deadline_at` is null (these represent parsing failures)

2. **Initialize agent:**
   - Create CodingAgentWorkflow instance
   - Configure with Gemini LLM and prompts
   - Set error_queue_path, parsers_dir, tests_dir

3. **Run REASON node:**
   - Execute workflow up to REASON node
   - Verify: Errors are read from queue
   - Verify: LLM clusters errors correctly
   - Verify: Selected clusters are chosen (up to CLUSTER_BATCH_SIZE)
   - Verify: State contains error_clusters and selected_clusters

### Test 4: Agent Workflow - PLAN Node

**Steps:**

1. **Continue from Test 3:**
   - Run PLAN node with cluster analysis from REASON node

2. **Verify planning:**
   - Verify: LLM creates code plans for each selected cluster
   - Verify: Plans include parsing_strategy, code_structure, test_cases, dependencies
   - Verify: State contains code_plan

### Test 5: Agent Workflow - ACT Node

**Steps:**

1. **Continue from Test 4:**
   - Run ACT node with code plan from PLAN node

2. **Verify code generation:**
   - Verify: Cluster module files are written to parsers/ directory
   - Verify: Test files are written to tests/ directory
   - Verify: Files are syntactically correct Python
   - Verify: Module files export parse() function
   - Verify: Test files use pytest with parameterized tests

3. **Verify file locking:**
   - Test: Files are written atomically
   - Test: No race conditions during file writing

### Test 6: Agent Workflow - VALIDATE Node

**Steps:**

1. **Continue from Test 5:**
   - Run VALIDATE node after ACT node

2. **Verify test execution:**
   - Verify: pytest runs on tests/ directory
   - Verify: Test results are parsed correctly
   - Verify: all_passed flag is set correctly
   - Verify: retry_count is incremented

3. **Test retry logic:**
   - If tests fail, verify workflow loops back to PLAN node
   - Verify retry_count increments correctly
   - After MAX_RETRY_ATTEMPTS, verify failed batch is logged to failed_batches.jsonl

### Test 7: Complete End-to-End Flow

**Steps:**

1. **Start with empty parsers/ directory:**
   - Clear any existing cluster modules

2. **Populate error queue:**
   - Load fixture data from `tests/fixtures/follow_up_tasks_202512121435.jsonl`
   - Filter for rows where `deadline_at` is null (parsing failures)
   - Add at least ERROR_THRESHOLD errors to error_queue.jsonl
   - Use diverse error patterns (relative dates, specific dates, time ranges)

3. **Run complete workflow:**
   - Initialize and run CodingAgentWorkflow
   - Let workflow complete (REASON → PLAN → ACT → VALIDATE → success)

4. **Verify success:**
   - Verify: Cluster modules are created in parsers/
   - Verify: Test files are created in tests/
   - Verify: All tests pass
   - Verify: Processed errors are removed from error_queue.jsonl
   - Verify: Parser can be reloaded and works with new modules

5. **Test parser with new capabilities:**
   - Reload parser using `coding_agent.reloader.reload_parser()`
   - Test parser with inputs that previously failed
   - Verify: Parser now handles these inputs correctly

### Test 8: Safety Valve (Failed Batch Handling)

**Steps:**

1. **Create scenario where tests will fail:**
   - Manually corrupt a generated test file or module file
   - Or use prompts that generate invalid code

2. **Run workflow:**
   - Let workflow retry up to MAX_RETRY_ATTEMPTS

3. **Verify safety valve:**
   - Verify: After max retries, failed batch is logged to failed_batches.jsonl
   - Verify: Workflow ends with "failure" status (does not block)
   - Verify: Error queue still contains original errors (not removed)
   - Verify: Agent can continue with next batch

### Troubleshooting

**Issue: LLM returns invalid JSON**
- Check: Prompt templates are correctly formatted
- Check: LLM response parsing handles markdown code blocks
- Solution: Improve JSON extraction in `_parse_json_response()`

**Issue: Generated code has syntax errors**
- Check: ACT node prompt requires complete, syntactically correct code
- Check: LLM is using correct temperature (0.1-0.3)
- Solution: Refine ACT node prompt, add syntax validation before writing files

**Issue: Tests fail after code generation**
- Check: Test assertions match actual parser behavior
- Check: Expected datetime values are correct (relative to "now")
- Solution: Refine PLAN node to create better test expectations, or adjust ACT node to generate better code

**Issue: Module reloading doesn't work**
- Check: importlib.reload() is called correctly
- Check: Module paths are correct
- Solution: Verify reloader.py implementation, check sys.modules state

**Issue: Error queue cleanup removes wrong errors**
- Check: Error indices mapping is correct (cluster indices to global indices)
- Solution: Verify index mapping logic in queue cleanup

## Future Work

### Parallel Processing Opportunities

Currently, the workflow processes clusters sequentially. Future enhancements could include:

1. **Parallel Cluster Processing:**
   - After REASON node, process multiple clusters in parallel
   - Each cluster gets its own PLAN/ACT/VALIDATE chain
   - Merge results before final queue cleanup

2. **Parallel Test Execution:**
   - Run tests for different clusters in parallel
   - Use pytest-xdist for parallel test execution

3. **Parallel Code Generation:**
   - Generate parser modules and test files in parallel
   - Each cluster is independent, so parallel generation is safe

### Audit Trail for Parser Changes

Currently, parser modules are overwritten without backup. Future enhancement:

1. **Version Control Integration:**
   - Keep history of parser module versions
   - Track which errors triggered which changes
   - Enable rollback to previous versions
   - Similar to audit trail patterns in existing codebase

### Enhanced Error Handling

1. **Better Error Recovery:**
   - Detect and handle specific error types (syntax errors, import errors, etc.)
   - Provide more specific feedback to LLM for retry attempts

2. **Confidence Scoring:**
   - Add confidence scores to generated code
   - Only deploy high-confidence changes automatically
   - Require human review for low-confidence changes

### Performance Monitoring

1. **Metrics Collection:**
   - Track parsing success rate over time
   - Monitor agent performance (clustering accuracy, code generation quality)
   - Track test pass rates and retry frequencies

2. **Performance Optimization:**
   - Cache LLM responses for similar error patterns
   - Optimize module loading and reloading
   - Reduce redundant code generation

### Production Deployment Considerations

Before production deployment, address:

1. **Security:**
   - Validate generated code before execution
   - Sandbox code execution if possible
   - Review generated code for security issues

2. **Reliability:**
   - Add comprehensive error handling
   - Implement circuit breakers for LLM API calls
   - Add monitoring and alerting

3. **Scalability:**
   - Handle large error queues efficiently
   - Implement batching and pagination for error processing
   - Optimize for high-throughput scenarios

