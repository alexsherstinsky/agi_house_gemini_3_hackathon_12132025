# Development Plan Sprint 1: Self-Healing Time Parser - Foundation and LangGraph Integration

## Overview

This sprint implements a self-healing time parser system that uses an LLM-based coding agent to automatically update parsing logic when encountering new time expression patterns. The system learns from failures and improves itself without manual intervention. This sprint establishes the foundation (MVP) and integrates LangGraph workflow with REASON/PLAN/ACT/VALIDATE pattern. The implementation uses Google Gemini 3 via LangChain, implements modular parser architecture with cluster-specific modules, and includes comprehensive test-driven validation.

## Critical Directives

1. **Confirm before marking tasks** - Never mark a task as complete with checkmark (✓) without explicit user permission.
2. **Ask permission to proceed** - After marking a task complete, state the next task and request permission before continuing.
3. **Never proceed without permission** - Always wait for explicit approval before starting any task.
4. **Follow specification exactly** - Adhere strictly to this design specification and referenced design documents.
5. **Follow coding standards** - Obey all Python coding standards (PEP 8, type hints, docstrings).
6. **Pure ASCII markdown** - Keep all sprint documentation in ASCII-only markdown format.
7. **Test after each step** - Run tests and manual verification for affected components after implementing each functional chunk.
8. **Preserve existing functionality** - Ensure no regressions in existing codebase utilities.
9. **Modular architecture** - Use modular architecture with one module per error cluster (not single large file).
10. **UTC timezone requirement** - All datetime objects must be timezone-aware with UTC timezone.

## Sprint Goals

- Install project dependencies via pyproject.toml.
- Create TimeParser class with dynamic cluster module loading.
- Implement exception interceptor/wrapper for error logging.
- Create error queue management (JSONL format with append/read/remove functionality).
- Implement prompt templates for REASON, PLAN, and ACT nodes.
- Create CodingAgentWorkflow class extending WorkflowBase with LangGraph R/P/A pattern.
- Implement REASON node for error clustering using LLM.
- Implement PLAN node for code planning using LLM.
- Implement ACT node for code generation using LLM.
- Implement VALIDATE node for pytest test execution.
- Add retry logic with conditional edges and safety valve (failed batch logging).
- Create demo Jupyter notebook with complete self-healing flow demonstration.
- Test complete end-to-end flow (error collection → clustering → code generation → validation → parser update).

## Implementation Clarifications

### Module Paths and Locations

1. **Time Parser Module:**
   - Main parser: `time_parser/parser.py` (orchestrator, rarely updated)
   - Cluster modules: `time_parser/parsers/<cluster_id>.py` (generated by agent, one per cluster)
   - Exception wrapper: `time_parser/wrapper.py` (error interceptor)

2. **Test Directory:**
   - Test files: `time_parser/tests/test_<cluster_id>.py` (one per cluster module)
   - Shared utilities: `time_parser/tests/test_utils.py`
   - Shared fixtures: `time_parser/tests/conftest.py`

3. **Coding Agent:**
   - Main workflow: `coding_agent/agent.py` (CodingAgentWorkflow class)
   - Configuration: `coding_agent/config.py` (constants: CLUSTER_BATCH_SIZE, MAX_RETRY_ATTEMPTS, ERROR_THRESHOLD)
   - Queue management: `coding_agent/error_queue.py` (read, append, remove functions)
   - Test runner: `coding_agent/test_runner.py` (pytest integration)
   - Module reloader: `coding_agent/reloader.py` (dynamic module reloading)
   - Prompt templates: `coding_agent/prompts.py` (add prompt template strings)

4. **Utilities:**
   - LLM helpers: `utils/llm_helpers.py` (call_llm_with_prompt function - already exists)
   - Dynamic loading: `utils/dynamic_loading.py` (module loading utilities - already exists)
   - JSON parser: `utils/llm_json_parser.py` (LLMJsonParser class for parsing LLM JSON responses - already exists)
   - JSON helpers: `utils/json_helpers.py` (is_valid_json function - already exists)
   - LLM output parsing utilities: `utils/llm_output_parsing_util.py` (helper functions to extract structured data from parsed JSON responses - to be created)

5. **Demo Notebook:**
   - File: `notebooks/demo.ipynb` (Jupyter notebook for demonstration)

6. **Runtime Files:**
   - Error queue: `error_queue.jsonl` (repository root, gitignored)
   - Failed batches: `failed_batches.jsonl` (repository root, gitignored)

7. **Test Fixtures:**
   - Error examples: `tests/fixtures/follow_up_tasks_202512121435.jsonl` (source data for testing and demonstration)

### Architectural Decisions

1. **Modular Parser Architecture:**
   - Main parser (`time_parser/parser.py`) orchestrates by discovering and loading cluster-specific modules
   - Each error cluster gets its own module file in `time_parser/parsers/`
   - Each module exports a `parse(text: str) -> datetime | None` function
   - Main parser tries each cluster module in sequence until one succeeds
   - No limit on number of clusters - scales naturally with new module files

2. **LLM Integration:**
   - Use Google Gemini 3 via LangChain (`langchain_google_genai.ChatGoogleGenerativeAI`)
   - API key from `GOOGLE_API_KEY` environment variable
   - Use `call_llm_with_prompt()` from `utils/llm_helpers.py` with `schema=None` (schema-less output)
   - Parse JSON responses manually from LLM text output
   - Temperature: 0.1-0.3 for code generation
   - Model: "gemini-3" (or "gemini-2.5-flash" as fallback)

3. **LangGraph Workflow Pattern:**
   - Use `WorkflowBase` from `coding_agent/base.py` as base class
   - Use `AnnotationState` for workflow state management (no custom state class needed)
   - Custom data stored in `state["node_output"]` as a dict (e.g., `state["node_output"]["error_clusters"]`)
   - Nodes: REASON → PLAN → ACT → VALIDATE
   - Conditional edge from VALIDATE: success → END, retry → PLAN, failure → END (safety valve)
   - Node methods signature: `def _node_<name>(self, state: AnnotationState) -> AnnotationState:`
   - Access state: `state["node_output"]` (dict), `state["messages"]`, `state["final_output"]`
   - Update state: Modify `state["node_output"]` dict, call `self._update_state(state, messages=[...], node_output=node_output)` (modifies state in place, returns None), then return state
   - JSON parsing: Use `self._llm_json_parser.parse_llm_json_extraction_response()` (available from WorkflowBase), returns `list[dict[str, Any]] | None`
     - This method handles: markdown code block extraction, JSON repair, multiple JSON objects, returns list of dicts or None
     - Input: raw LLM response string (may contain markdown, multiple JSON objects, malformed JSON)
     - Output: `list[dict[str, Any]] | None` - list of parsed JSON objects, or None if parsing fails completely
   - JSON extraction: Use helper functions from `utils.llm_output_parsing_util` to extract structured data from parsed responses
   - Prompts: Access via `self._node_prompts.<node_name>` (returns SystemAndUserPromptPair with .system_prompt and .user_prompt)
   - Prompt formatting: System prompts are constants, user prompts are templates formatted in each node method using `.format()` (not at initialization)
   - LLM calls: Use `self._call_llm_with_prompt(node_name="...", system_prompt=..., user_prompt=..., schema=None)` - always use `schema=None`
     - Note: `schema` parameter is accepted but ignored when `enforce_structured_llm_output=False` (which should be False for this workflow)
     - Response is always `AIMessage` with `.content` attribute (string) when `schema=None`
   - Initial state: `AnnotationState(messages=[], node_output=None, final_output=None)` - pass to `workflow.run(initial_state=...)`
   - Thread ID: Generate unique ID (e.g., `f"coding_agent_{uuid.uuid4().hex[:8]}"` or timestamp-based) - required for WorkflowBase.__init__()

4. **Error Queue Format:**
   - JSONL format (one JSON object per line)
   - Fields: `timing_description` (primary), `auxiliary_pretty` (optional), `customer_id`, `deadline_at`
   - Easy to append, read batch, and remove processed items

5. **Error Clustering:**
   - LLM-based semantic clustering in REASON node
   - Process up to CLUSTER_BATCH_SIZE clusters per batch (default: 3, configurable via `coding_agent.config.CLUSTER_BATCH_SIZE`)
   - Note: Configuration constant is `CLUSTER_BATCH_SIZE` (not ERROR_BATCH_SIZE) - defined in `coding_agent/config.py`
   - Prioritize parsable clusters over context-dependent or ambiguous ones
   - Cluster IDs become module filenames (lowercase_with_underscores, sanitized)

6. **Code Generation:**
   - ACT node generates complete module files (not snippets)
   - One module file per cluster: `time_parser/parsers/<cluster_id>.py`
   - One test file per cluster: `time_parser/tests/test_<cluster_id>.py`
   - All code must be syntactically correct and ready to execute
   - Store generated modules and test files as dictionaries with cluster_id (without `.py` suffix) as keys

7. **Test Validation:**
   - VALIDATE node runs pytest on `time_parser/tests/` directory
   - All tests must pass before proceeding to module reload
   - If tests fail, loop back to PLAN node (up to MAX_RETRY_ATTEMPTS)
   - After max retries, log failed batch to `failed_batches.jsonl` and give up

8. **Module Reloading:**
   - After successful validation, reload cluster modules using `importlib.reload()`
   - Call `TimeParser.reload_cluster_modules()` to refresh parser registry
   - Use file locking (fcntl) to prevent race conditions during updates (Unix/macOS/Linux only)

9. **Exception Interceptor:**
   - Decorator/context manager pattern that wraps parser calls
   - Catches exceptions, logs to error queue, re-raises or returns None
   - Does not block original code execution

10. **Safety Valve:**
    - Failed batches (after max retries) logged to `failed_batches.jsonl`
    - Agent continues with next batch (does not block processing)
    - Failed batches can be reviewed manually later

## Sprint Tasks

### 1.0 Install Dependencies and Verify Environment [S]

   1.1 **Install project dependencies** [S]
       - Navigate to project root: `cd agi_house_gemini_3_hackathon_12132025`
       - Run: `pip install -e .` (installs main dependencies including pandas)
       - Optional: Run `pip install -e ".[dev]"` to also install dev dependencies (jupyter, matplotlib, tqdm)
       - Verify all dependencies install correctly (langchain, langchain-google-genai, langgraph, pydantic, pytest, python-dateutil, pandas)
       - Note: This installs from pyproject.toml

   1.2 **Verify GOOGLE_API_KEY environment variable** [S]
       - Check: `GOOGLE_API_KEY` is set in environment
       - Test: Create simple test script to verify Gemini API access
       - Note: API key should be available before proceeding with LLM integration tasks

### 2.0 Create TimeParser Class with Dynamic Module Loading [M]

   2.1 **Create TimeParser class** [M]
       - File: `time_parser/parser.py`
       - Implement `__init__()`: Initialize `_cluster_parsers` dict and call `_load_cluster_modules()`
       - Implement `_load_cluster_modules()`: Discover all `.py` files in `parsers/` directory, import each module, register `parse()` function
       - Implement `reload_cluster_modules()`: Reload existing modules using `importlib.reload()`, then call `_load_cluster_modules()`
       - Implement `parse(text: str) -> datetime`: Try each cluster parser in order, return first successful result or raise ValueError
       - Handle errors gracefully: If a cluster module fails to load, continue with other modules
       - Test: Verify parser loads modules correctly, handles missing modules gracefully

   2.2 **Create initial basic parser for testing** [S]
       - Initially, parser should handle basic cases: "asap", "now" (for testing)
       - These can be hardcoded in `parse()` method initially
       - Agent will generate cluster modules later
       - Test: Verify basic parsing works before agent integration

### 3.0 Create Exception Interceptor/Wrapper [M]

   3.1 **Create intercept_parser_errors decorator** [M]
       - File: `time_parser/wrapper.py`
       - Implement decorator pattern: `intercept_parser_errors(parser_instance)`
       - Behavior: Catch exceptions from parser, log to error queue (JSONL format), re-raise exception
       - Error entry format: Must have `timing_description` (required, str), `deadline_at` (required, None for failures), `customer_id` (optional, can be None), `auxiliary_pretty` (optional, can be None or JSON string)
       - Validation: Before appending, validate that `timing_description` is a non-empty string, `deadline_at` is None (for failures)
       - Error entry: `{"customer_id": customer_id or None, "deadline_at": None, "timing_description": text, "auxiliary_pretty": json.dumps({...}) if auxiliary_data else None}`
       - Use `coding_agent.error_queue.append_error_to_queue()` for logging
       - Test: Verify errors are logged correctly to error_queue.jsonl

   3.2 **Test exception interceptor with TimeParser** [S]
       - Create test script that uses wrapped parser
       - Test with inputs that fail parsing
       - Verify errors appear in error_queue.jsonl
       - Verify original code execution continues (does not block)

### 4.0 Implement Prompt Templates [M]

   4.1 **Add REASON node prompt templates** [M]
       - File: `coding_agent/prompts.py`
       - Add: `REASON_NODE_SYSTEM_PROMPT_TEMPLATE` (from Section 18.1 of design doc) - constant string
       - Add: `REASON_NODE_USER_PROMPT_TEMPLATE` (from Section 18.1 of design doc) - template with `{error_queue_contents}` placeholder
       - Note: System prompt is constant, user prompt is formatted in REASON node with actual error queue contents
       - Note: `{error_queue_contents}` should contain entire content of parsing errors (only rows where parsing failed, i.e., `deadline_at` is null)
       - Test: Verify templates are valid Python strings, can be formatted

   4.2 **Add PLAN node prompt templates** [M]
       - File: `coding_agent/prompts.py`
       - Add: `PLAN_NODE_SYSTEM_PROMPT_TEMPLATE` (from Section 18.2 of design doc) - constant string
       - Add: `PLAN_NODE_USER_PROMPT_TEMPLATE` (from Section 18.2 of design doc) - template with `{cluster_analysis}` and `{existing_cluster_modules}` placeholders
       - Note: System prompt is constant, user prompt is formatted in PLAN node with cluster analysis and existing modules
       - Note: `{cluster_analysis}` is the output from REASON node, `{existing_cluster_modules}` are modules already in `time_parser/parsers/` directory (for context)
       - Test: Verify templates are valid Python strings, can be formatted

   4.3 **Add ACT node prompt templates** [M]
       - File: `coding_agent/prompts.py`
       - Add: `ACT_NODE_SYSTEM_PROMPT_TEMPLATE` (from Section 18.3 of design doc) - constant string
       - Add: `ACT_NODE_USER_PROMPT_TEMPLATE` (from Section 18.3 of design doc) - template with `{code_plan}` placeholder
       - Note: System prompt is constant, user prompt is formatted in ACT node with code plan
       - Note: `{code_plan}` is the output from PLAN node
       - Test: Verify templates are valid Python strings, can be formatted

   4.4 **Create helper function to build NodePrompts** [S]
       - File: `coding_agent/prompts.py`
       - Add function: `build_node_prompts() -> NodePrompts`
       - Function signature: `def build_node_prompts() -> NodePrompts:`
       - Implementation: Create SystemAndUserPromptPair instances using the constant system prompts and template user prompts (not formatted)
       - Returns NodePrompts with reason, plan, act prompts populated
       - Example structure:
         ```python
         def build_node_prompts() -> NodePrompts:
             """Build NodePrompts with template user prompts (not formatted)."""
             return NodePrompts(
                 reason=SystemAndUserPromptPair(
                     system_prompt=REASON_NODE_SYSTEM_PROMPT_TEMPLATE,
                     user_prompt=REASON_NODE_USER_PROMPT_TEMPLATE,  # Template, not formatted
                 ),
                 plan=SystemAndUserPromptPair(
                     system_prompt=PLAN_NODE_SYSTEM_PROMPT_TEMPLATE,
                     user_prompt=PLAN_NODE_USER_PROMPT_TEMPLATE,  # Template, not formatted
                 ),
                 act=SystemAndUserPromptPair(
                     system_prompt=ACT_NODE_SYSTEM_PROMPT_TEMPLATE,
                     user_prompt=ACT_NODE_USER_PROMPT_TEMPLATE,  # Template, not formatted
                 ),
                 validate=None,  # VALIDATE node doesn't use LLM
             )
         ```
       - Note: This function is called at workflow initialization to build NodePrompts
       - Note: User prompts are templates with placeholders (e.g., `{error_queue_contents}`, `{cluster_analysis}`) - they are formatted in each node method (not at initialization)
       - In nodes, access prompts via `self._node_prompts.<node_name>` (returns SystemAndUserPromptPair), then format user prompt with `.format()` in the node method
       - Test: Verify function creates valid NodePrompts object with template user prompts

### 5.0 Create CodingAgentWorkflow Class Structure [M]

   5.0a **Create cluster ID sanitization helper** [S]
       - File: `coding_agent/agent.py`
       - Add imports: `import re`, `import keyword`
       - Add function: `_sanitize_cluster_id(cluster_id: str) -> str`
       - Steps:
         1. Convert to lowercase: `cluster_id = cluster_id.lower()`
         2. Replace spaces and special characters with underscores: Use regex `re.sub(r'[^a-z0-9_]', '_', cluster_id)`
         3. Remove leading/trailing underscores: `cluster_id = cluster_id.strip('_')`
         4. Replace multiple consecutive underscores with single underscore: `re.sub(r'_+', '_', cluster_id)`
         5. Ensure starts with letter or underscore: If first char is digit, prepend `cluster_`
         6. Validate: Check if result is empty or is a Python keyword (use `keyword.iskeyword()`)
         7. If empty or keyword, append `_cluster` suffix
         8. Final validation: Ensure result is non-empty and valid Python identifier (use `str.isidentifier()`)
         9. Return sanitized cluster_id
       - Test: Verify sanitization handles various inputs correctly (spaces, special chars, keywords, empty strings)

   5.1 **Create CodingAgentWorkflow class skeleton** [M]
       - File: `coding_agent/agent.py`
       - Extend `WorkflowBase` from `coding_agent.base`
       - Implement `__init__()`: Accept parameters:
         - `error_queue_path: str | Path` - Path to error queue JSONL file
         - `parsers_dir: str | Path` - Directory for cluster parser modules
         - `tests_dir: str | Path` - Directory for test files
         - `node_llms: NodeLLMs` - LLM configuration for each node
         - `node_prompts: NodePrompts` - Prompt templates for each node
         - `thread_id: str` - Unique thread identifier
         - `rate_limiting_config: RateLimitingConfig = DEFAULT_RATE_LIMITING_CONFIG` - Rate limiting config (from coding_agent.base)
         - `fail_fast: bool = False` - Whether to fail fast on errors
         - `error_logging: bool = True` - Whether to log errors
         - `debug_logging: bool = False` - Whether to enable debug logging
         - `enforce_structured_llm_output: bool = False` - Whether to enforce structured output (should be False for schema=None)
       - Store configuration as instance variables: `self._error_queue_path`, `self._parsers_dir`, `self._tests_dir`
       - Call `super().__init__()` with: `node_llms`, `node_prompts`, `thread_id`, `enforce_structured_llm_output`, `rate_limiting_config`, `fail_fast`, `error_logging`, `debug_logging`
       - Import required dependencies: WorkflowBase, AnnotationState, NodeLLMs, NodePrompts, RateLimitingConfig, DEFAULT_RATE_LIMITING_CONFIG, etc.
       - Note: No custom state class needed - use `AnnotationState` and store custom data in `state["node_output"]` dict
       - Note: error_queue_path, parsers_dir, tests_dir are instance variables (not stored in state)
       - Test: Verify class can be instantiated

   5.2 **Implement _add_workflow_nodes_and_edges method** [M]
       - File: `coding_agent/agent.py`
       - Required imports: `from langgraph.graph import END, START, StateGraph` (START and END are constants from langgraph.graph)
       - Add nodes: `workflow.add_node("reason", self._reason_node)`, etc.
       - Set entry point: `workflow.add_edge(START, "reason")` (START is imported from langgraph.graph)
       - Add sequential edges: `workflow.add_edge("reason", "plan")`, `workflow.add_edge("plan", "act")`, `workflow.add_edge("act", "validate")`
       - Add conditional edge from validate: `workflow.add_conditional_edges("validate", self._should_retry, {"success": END, "retry": "plan", "failure": END})` (END is imported from langgraph.graph)
       - Add conditional edge from reason: `workflow.add_conditional_edges("reason", self._check_early_exit, {"continue": "plan", "exit": END})` (check for early_exit flag in state)
       - Add conditional edge from plan: `workflow.add_conditional_edges("plan", self._check_early_exit, {"continue": "act", "exit": END})` (check for early_exit flag in state)
       - Add conditional edge from act: `workflow.add_conditional_edges("act", self._check_early_exit, {"continue": "validate", "exit": END})` (check for early_exit flag in state)
       - Note: `_should_retry()` method should return "success", "retry", or "failure" based on test results and retry count
       - Note: `_check_early_exit()` method should return "exit" if `state["node_output"].get("early_exit")` is True, otherwise "continue"
       - Test: Verify workflow graph structure is correct

   5.3 **Implement _check_early_exit helper method** [S]
       - File: `coding_agent/agent.py`
       - Method signature: `def _check_early_exit(self, state: AnnotationState) -> str:`
       - Check if early exit flag is set: `if state["node_output"].get("early_exit") is True:`
       - If True, return "exit" (workflow routes to END)
       - Otherwise, return "continue" (workflow continues to next node)
       - Test: Verify early exit routing works correctly

### 6.0 Implement REASON Node [M]

   6.1 **Implement _reason_node method** [M]
       - File: `coding_agent/agent.py`
       - Method signature: `def _reason_node(self, state: AnnotationState) -> AnnotationState:`
       - Read full error queue using `coding_agent.error_queue.read_error_queue(self._error_queue_path)` - store as `all_errors`
       - Filter errors: Only include rows where `deadline_at` is null (parsing failures - these are the errors we want to fix) - store as `filtered_errors`
       - Create mapping from filtered index to global index: Build dict mapping filtered_errors index to all_errors index:
         ```python
         filtered_to_global = {}
         filtered_idx = 0
         for global_idx, error in enumerate(all_errors):
             if error.get("deadline_at") is None:
                 filtered_to_global[filtered_idx] = global_idx
                 filtered_idx += 1
         ```
       - Error handling: If no errors found (filtered_errors is empty), log warning and set early exit:
         - Set `node_output["error_clusters"] = []` (empty list)
         - Set `node_output["selected_clusters"] = []` (empty list)
         - Set `node_output["cluster_error_indices"] = {}` (empty dict)
         - Set `node_output["early_exit"] = True` (flag for early exit)
         - Set `state["final_output"] = {"success": True, "processed_clusters": [], "errors_removed_count": 0, "parser_updated": False, "tests_passed": False, "retry_count": 0, "message": "No errors to process"}`
         - Return state (workflow will route to END via conditional edge check)
       - Format error queue contents as JSONL string: `error_queue_contents = "\n".join(json.dumps(error) for error in filtered_errors)`
       - Required imports: `import json` (for json.dumps)
       - Note: Each line in error_queue_contents should be a complete JSON object with `timing_description` field (the text that failed to parse)
       - Get prompts: `system_and_user_prompt_pair = self._node_prompts.reason`
       - Extract: `system_prompt = system_and_user_prompt_pair.system_prompt` (constant, no formatting)
       - Format user prompt in node: `user_prompt = system_and_user_prompt_pair.user_prompt.format(error_queue_contents=error_queue_contents)`
       - Call LLM: `response = self._call_llm_with_prompt(node_name="reason", system_prompt=system_prompt, user_prompt=user_prompt, schema=None)`
       - Note: When `schema=None`, response is always `AIMessage` with `.content` attribute (string)
       - Error handling: If LLM call fails, log error and raise exception (or handle based on fail_fast setting)
       - Parse JSON: Use `self._llm_json_parser.parse_llm_json_extraction_response(response.content, fail_fast=self._fail_fast, context_identifier=("Node", "reason"), debug_logging=self._debug_logging)`
       - Returns: `list[dict[str, Any]] | None`
       - Error handling: If parsed_response is None or empty, log error and raise ValueError (or handle based on fail_fast)
       - Extract data: Use helper function from `utils.llm_output_parsing_util.extract_reason_node_output(parsed_response)` to extract clusters and selected_clusters
       - Error handling: If helper function returns None or missing required keys, log error and raise ValueError (or handle based on fail_fast setting)
       - Sanitize cluster IDs immediately after extraction: For each cluster in extracted_data["clusters"], sanitize cluster_id using `self._sanitize_cluster_id(cluster["cluster_id"])` and update cluster dict in place
       - Sanitize selected_clusters: Apply `self._sanitize_cluster_id()` to each cluster_id in extracted_data["selected_clusters"] and filter to only include clusters that exist in sanitized clusters
       - Note: Cluster IDs must be sanitized immediately after extraction from LLM response, before storing in state (sanitized IDs are used as module filenames)
       - Map error indices from filtered to global: For each cluster in extracted_data["clusters"], convert error_indices (which are relative to filtered_errors) to global indices: `cluster["error_indices"] = [filtered_to_global[idx] for idx in cluster["error_indices"]]`
       - Store in state: `node_output = state["node_output"] or {}`, set `node_output["error_clusters"]` (list of cluster dicts with sanitized cluster_id, error_indices [now global], commonality, examples, etc.) and `node_output["selected_clusters"]` (list of sanitized cluster_ids)
       - Also store `node_output["cluster_error_indices"]` as dict mapping sanitized cluster_id to list of global error indices: `{cluster["cluster_id"]: cluster["error_indices"] for cluster in extracted_data["clusters"] if cluster["cluster_id"] in extracted_data["selected_clusters"]}`
       - Construct messages for state update: `from langchain_core.messages import SystemMessage, HumanMessage; system_message = SystemMessage(content=system_prompt); human_message = HumanMessage(content=user_prompt); messages = [system_message, human_message, response]`
       - Call `self._update_state(state, messages=messages, node_output=node_output)` (modifies state in place, returns None)
       - Return state: `return state` (state is modified in place by _update_state, just return it)
       - Error handling: If any step fails, log error and either raise exception (if fail_fast=True) or log warning and return state with error indicators (if fail_fast=False)
       - Test: Verify REASON node clusters errors correctly with mock LLM response

### 7.0 Implement PLAN Node [M]

   7.1 **Implement _plan_node method** [M]
       - File: `coding_agent/agent.py`
       - Method signature: `def _plan_node(self, state: AnnotationState) -> AnnotationState:`
       - Read existing cluster modules from parsers_dir: List `.py` files (excluding `__init__.py`), read each file, extract brief description
       - Description extraction logic:
         1. Read file content
         2. Try to extract module docstring: Look for triple-quoted string at module level (first statement after imports)
         3. If docstring found: Extract first sentence or first 100 characters (whichever is shorter), strip whitespace
         4. If no docstring: Extract first non-empty, non-import, non-comment line (max 100 chars), strip whitespace
         5. If still empty: Use default description: `"Parser module for {module_name} patterns"`
         6. Truncate description to max 150 characters
       - Format existing_cluster_modules: Create list of dicts with structure: `[{"module_name": "relative_dates", "description": "Brief description extracted from module docstring or first few lines"}]`
       - Format as string: `existing_modules_str = json.dumps(existing_cluster_modules, indent=2)`
       - Required imports: `import json` (for json.dumps)
       - Note: Module names should be unambiguous without `.py` suffix (e.g., `relative_dates` not `relative_dates.py`)
       - Check for early exit: If `state["node_output"].get("early_exit")` is True, return state unchanged (workflow will route to END)
       - Format cluster_analysis from state: Get full cluster data from `state["node_output"]["error_clusters"]`, filter by `state["node_output"]["selected_clusters"]` (list of cluster_ids)
       - Create cluster_analysis dict: For each selected cluster_id, get the full cluster dict from error_clusters (includes cluster_id, error_indices, commonality, examples, suggested_approach, parsability, error_count)
       - Format as string: `cluster_analysis = json.dumps([cluster for cluster in error_clusters if cluster["cluster_id"] in selected_clusters], indent=2)`
       - Error handling: If selected_clusters is empty or None, log warning and set early exit:
         - Set `node_output["code_plan"] = {}` (empty dict)
         - Set `node_output["early_exit"] = True`
         - Set `state["final_output"] = {"success": True, "processed_clusters": [], "errors_removed_count": 0, "parser_updated": False, "tests_passed": False, "retry_count": 0, "message": "No clusters selected for processing"}`
         - Return state (workflow will route to END)
       - Get prompts: `system_and_user_prompt_pair = self._node_prompts.plan`
       - Extract: `system_prompt = system_and_user_prompt_pair.system_prompt` (constant, no formatting)
       - Format user prompt in node: `user_prompt = system_and_user_prompt_pair.user_prompt.format(cluster_analysis=cluster_analysis, existing_cluster_modules=existing_modules_str)`
       - Call LLM: `response = self._call_llm_with_prompt(node_name="plan", system_prompt=system_prompt, user_prompt=user_prompt, schema=None)`
       - Note: When `schema=None`, response is always `AIMessage` with `.content` attribute (string)
       - Error handling: If LLM call fails, log error and raise exception (or handle based on fail_fast setting)
       - Parse JSON: Use `self._llm_json_parser.parse_llm_json_extraction_response(response.content, fail_fast=self._fail_fast, context_identifier=("Node", "plan"), debug_logging=self._debug_logging)`
       - Error handling: If parsed_response is None or empty, log error and raise ValueError
       - Extract code_plan: Use helper function from `utils.llm_output_parsing_util.extract_plan_node_output(parsed_response)` to extract code_plan dict (should have cluster_plans array)
       - Error handling: If helper function returns None or missing cluster_plans key, log error and raise ValueError
       - Error handling: If `code_plan.get("cluster_plans")` is empty list, log error and raise ValueError with message "code_plan has empty cluster_plans array"
       - Update state: `node_output = state["node_output"] or {}`, set `node_output["code_plan"] = code_plan`
       - Construct messages for state update: `from langchain_core.messages import SystemMessage, HumanMessage; system_message = SystemMessage(content=system_prompt); human_message = HumanMessage(content=user_prompt); messages = [system_message, human_message, response]`
       - Call `self._update_state(state, messages=messages, node_output=node_output)` (modifies state in place, returns None)
       - Return state: `return state` (state is modified in place by _update_state, just return it)
       - Error handling: If any step fails, log error and either raise exception (if fail_fast=True) or log warning and return state with error indicators (if fail_fast=False)
       - Test: Verify PLAN node creates code plans correctly with mock LLM response

### 8.0 Implement ACT Node [M]

   8.1 **Implement _act_node method** [M]
       - File: `coding_agent/agent.py`
       - Method signature: `def _act_node(self, state: AnnotationState) -> AnnotationState:`
       - Check for early exit: If `state["node_output"].get("early_exit")` is True, return state unchanged (workflow will route to END)
       - Get code_plan from state: `code_plan = state["node_output"]["code_plan"]` (output from PLAN node)
       - Error handling: If code_plan is None or missing, log error and raise ValueError
       - Error handling: If code_plan exists but `code_plan.get("cluster_plans")` is empty list, log error and raise ValueError with message "code_plan has no cluster_plans to generate code for"
       - Format code_plan as JSON string: `code_plan_str = json.dumps(code_plan, indent=2)`
       - Required imports: `import json` (for json.dumps)
       - Get prompts: `system_and_user_prompt_pair = self._node_prompts.act`
       - Extract: `system_prompt = system_and_user_prompt_pair.system_prompt` (constant, no formatting)
       - Format user prompt in node: `user_prompt = system_and_user_prompt_pair.user_prompt.format(code_plan=code_plan_str)`
       - Call LLM: `response = self._call_llm_with_prompt(node_name="act", system_prompt=system_prompt, user_prompt=user_prompt, schema=None)`
       - Note: When `schema=None`, response is always `AIMessage` with `.content` attribute (string)
       - Error handling: If LLM call fails, log error and raise exception (or handle based on fail_fast setting)
       - Parse JSON: Use `self._llm_json_parser.parse_llm_json_extraction_response(response.content, fail_fast=self._fail_fast, context_identifier=("Node", "act"), debug_logging=self._debug_logging)`
       - Error handling: If parsed_response is None or empty, log error and raise ValueError
       - Extract cluster_modules and test_files: Use helper function from `utils.llm_output_parsing_util.extract_act_node_output(parsed_response)` to extract dictionaries from parsed response
       - Error handling: If helper function returns None or missing cluster_modules/test_files keys, log error and raise ValueError
       - Sanitize cluster IDs in extracted data: For each cluster_id key in cluster_modules and test_files, sanitize using `self._sanitize_cluster_id(cluster_id)` and update dictionaries (create new dicts with sanitized keys)
       - Store as dictionaries with sanitized cluster names (without `.py` suffix) as keys: `cluster_modules[sanitized_cluster_id] = code_string`, `test_files[sanitized_cluster_id] = test_code_string`
       - Validate cluster module structure: Each module must have:
         - Required imports: `from datetime import datetime, timezone` (use `timezone.utc` for UTC timezone, e.g., `datetime.now(timezone.utc)`) (or `from datetime import datetime, timedelta, timezone` if needed)
         - Module docstring: `"""Parser module for {cluster_id} patterns."""`
         - Function signature: `def parse(text: str) -> datetime | None:`
         - Function must return `datetime` object (timezone-aware, UTC) or `None` (not raise exception on failure)
         - Function must handle errors gracefully (return None, don't raise)
       - Validate test file structure: Each test file must have:
         - Required imports: `import pytest`, `from datetime import datetime, timezone`, `from time_parser.parsers.{cluster_id} import parse`
         - Test function naming: `def test_{cluster_id}_parsing():` or use `@pytest.mark.parametrize` for multiple test cases
         - Test structure: Use `pytest.mark.parametrize` decorator with test cases from code_plan
         - Assertions: Check that `parse(input_text)` returns non-None datetime, is timezone-aware, is UTC
         - Use `time_parser.tests.test_utils.assert_valid_datetime()` helper if available
       - Write cluster module files: For each sanitized cluster_id in cluster_modules, write to `Path(self._parsers_dir) / f"{sanitized_cluster_id}.py"`
         - Required imports: `from pathlib import Path` (for Path operations)
         - Note: If file already exists, it will be overwritten (new cluster modules replace old ones)
       - Write test files: For each sanitized cluster_id in test_files, write to `Path(self._tests_dir) / f"test_{sanitized_cluster_id}.py"`
         - Note: If test file already exists, it will be overwritten (new test files replace old ones)
         - Future enhancement: Could append new test cases to existing parameterized test lists, but for MVP, overwrite is acceptable
       - Use `_write_file_with_lock()` helper for atomic file writing
       - Error handling: If file writing fails, log error and raise IOError (or handle based on fail_fast)
       - Update state: `node_output = state["node_output"] or {}`, set `node_output["generated_cluster_modules"] = cluster_modules` and `node_output["generated_test_files"] = test_files` (both as dicts with cluster_id keys)
       - Cluster error indices already stored in REASON node: `node_output["cluster_error_indices"]` (dict mapping cluster_id to list of global error indices)
       - Construct messages for state update: `from langchain_core.messages import SystemMessage, HumanMessage; system_message = SystemMessage(content=system_prompt); human_message = HumanMessage(content=user_prompt); messages = [system_message, human_message, response]`
       - Call `self._update_state(state, messages=messages, node_output=node_output)` (modifies state in place, returns None)
       - Return state: `return state` (state is modified in place by _update_state, just return it)
       - Error handling: If any step fails (LLM call, JSON parsing, file writing), log error and either raise exception (if fail_fast=True) or log warning and return state with error indicators (if fail_fast=False)
       - Test: Verify ACT node generates and writes files correctly with mock LLM response

   8.2 **Add file writing helper with locking** [S]
       - File: `coding_agent/agent.py`
       - Add function: `_write_file_with_lock(file_path: Path, content: str) -> None`
       - Required imports: `import fcntl`, `import os`, `import time`
       - Implementation pattern (Unix/macOS/Linux only):
         1. Create temporary file: `temp_file = file_path.with_suffix(f"{file_path.suffix}.tmp")`
         2. Open temp file for writing: `with open(temp_file, "w", encoding="utf-8") as f:`
         3. Acquire exclusive lock: `fcntl.flock(f.fileno(), fcntl.LOCK_EX)` (blocking)
         4. Write content to temp file: `f.write(content)`
         5. Flush and sync: `f.flush()`, `os.fsync(f.fileno())`
         6. Release lock: `fcntl.flock(f.fileno(), fcntl.LOCK_UN)`
         7. Close file (automatic with context manager)
         8. Atomically rename: `temp_file.replace(file_path)` (atomic on Unix/macOS/Linux)
       - Error handling: If lock acquisition fails, wait and retry up to 3 times with 0.1s delay (`time.sleep(0.1)`), then raise IOError
       - Error handling: If write fails, remove temp file (`temp_file.unlink(missing_ok=True)`) and raise IOError
       - Error handling: If rename fails, remove temp file (`temp_file.unlink(missing_ok=True)`) and raise IOError
       - Note: Lock is held during entire write operation to prevent concurrent writes
       - Test: Verify file writing works correctly with locking on macOS/Linux

### 9.0 Implement VALIDATE Node [M]

   9.1 **Implement _validate_node method** [M]
       - File: `coding_agent/agent.py`
       - Method signature: `def _validate_node(self, state: AnnotationState) -> AnnotationState:`
       - Use `coding_agent.test_runner.run_pytest(self._tests_dir)` to run tests
       - Note: `run_pytest()` returns dict with keys: `all_passed` (bool), `test_output` (str), `test_errors` (str), `returncode` (int)
       - Error handling: If pytest execution fails (exception), log error and set test_results with all_passed=False
       - Parse test results: `test_results = run_pytest_result` (already in correct format)
       - Get current retry_count: `retry_count = state["node_output"].get("retry_count", 0)`
       - Increment retry_count: `retry_count += 1`
       - Update state: `node_output = state["node_output"] or {}`, set `node_output["test_results"] = test_results` and `node_output["retry_count"] = retry_count`
       - If tests pass (test_results["all_passed"] is True):
         - Perform queue cleanup first (see Task 10.1): Collect error indices from `node_output["cluster_error_indices"]`, flatten, remove from queue, calculate `errors_removed_count`
         - Perform module reloading (see Task 10.2): Call `coding_agent.reloader.reload_parser(self._parsers_dir)` (return value can be ignored - caller will create new TimeParser instance)
         - Get processed_clusters: `processed_clusters = node_output.get("selected_clusters", [])` (all selected clusters are processed if tests pass)
         - Calculate errors_removed_count: `errors_removed_count = sum(len(indices) for indices in node_output.get("cluster_error_indices", {}).values())`
         - Create final_output dict with structure matching design doc Section 4.2:
           - `final_output = {"success": True, "processed_clusters": processed_clusters, "errors_removed_count": errors_removed_count, "parser_updated": True, "tests_passed": True, "retry_count": retry_count, "generated_cluster_modules": node_output.get("generated_cluster_modules", {}), "generated_test_files": node_output.get("generated_test_files", {}), "cluster_error_indices": node_output.get("cluster_error_indices", {})}`
         - Set `state["final_output"] = final_output`
         - Update `node_output["errors_removed_count"] = errors_removed_count` and `node_output["parser_reloaded"] = True`
       - Update state: Set `state["node_output"] = node_output` directly (no messages needed for VALIDATE node - it doesn't call LLM)
       - Return state: `return state`
       - Error handling: If pytest execution fails with exception, catch it, log error, set test_results with all_passed=False, and either raise (if fail_fast=True) or continue (if fail_fast=False)
       - Test: Verify VALIDATE node runs pytest and parses results correctly

   9.2 **Implement _should_retry method** [S]
       - File: `coding_agent/agent.py`
       - Method signature: `def _should_retry(self, state: AnnotationState) -> str:`
       - Get test_results: `test_results = state["node_output"].get("test_results", {})`
       - Check test_results["all_passed"]: If True, return "success"
       - Get retry_count: `retry_count = state["node_output"].get("retry_count", 0)`
       - Check retry_count against MAX_RETRY_ATTEMPTS from `coding_agent.config`
       - If retry_count < MAX_RETRY_ATTEMPTS, return "retry"
       - Otherwise, call `self._log_failed_batch(state)` and return "failure"
       - Test: Verify retry logic works correctly for all scenarios

   9.3 **Implement _log_failed_batch method** [S]
       - File: `coding_agent/agent.py`
       - Method signature: `def _log_failed_batch(self, state: AnnotationState) -> None:`
       - Get data from state: `selected_clusters = state["node_output"].get("selected_clusters", [])`, `test_results = state["node_output"].get("test_results", {})`, `retry_count = state["node_output"].get("retry_count", 0)`, `cluster_error_indices = state["node_output"].get("cluster_error_indices", {})`
       - Get original errors: Read error queue and extract errors for failed clusters using cluster_error_indices
       - Create failed_batch_entry dict with structure:
         - `{"selected_clusters": list of cluster_ids, "cluster_error_indices": cluster_error_indices dict, "error_count": total error count, "test_results": test_results dict, "retry_count": retry_count, "timestamp": ISO timestamp string, "error_samples": list of sample error dicts (first 3 errors from each cluster)}`
         - For timestamp: Use `from datetime import datetime, timezone; datetime.now(timezone.utc).isoformat()` to generate ISO timestamp string
       - Append to failed_batches.jsonl using JSONL format: `coding_agent.error_queue.append_error_to_queue("failed_batches.jsonl", failed_batch_entry)`
       - Note: For hackathon MVP, failed_batches.jsonl path is hardcoded as "failed_batches.jsonl" (repository root, same directory as error_queue.jsonl)
       - Required imports: `import json` (for json.dumps in append_error_to_queue), `from datetime import datetime, timezone` (for timestamp generation)
       - Error handling: If file write fails, log warning but don't raise exception (non-critical)
       - Test: Verify failed batches are logged correctly

### 10.0 Implement Queue Cleanup and Module Reloading [M]

   10.1 **Add queue cleanup after successful validation** [M]
       - File: `coding_agent/agent.py`
       - Note: This is called from VALIDATE node (Task 9.1) after tests pass, before creating final_output
       - Collect error indices from `state["node_output"]["cluster_error_indices"]`
       - This dict maps cluster_id to list of error indices (global indices from original queue, mapped in REASON node)
       - Flatten all error indices from all clusters into a single list: `all_error_indices = [idx for indices in cluster_error_indices.values() for idx in indices]`
       - Use `coding_agent.error_queue.remove_processed_cluster_errors(self._error_queue_path, all_error_indices)` to remove errors
       - Note: `remove_processed_cluster_errors()` must use file locking to prevent race conditions (see Task 10.1a)
       - Calculate errors_removed_count: `errors_removed_count = len(all_error_indices)`
       - Note: Error indices are tracked through the workflow - REASON node maps filtered indices to global indices and stores in `cluster_error_indices` dict
       - Test: Verify processed errors are removed from queue correctly

   10.1a **Add file locking to queue cleanup operations** [S]
       - File: `coding_agent/error_queue.py`
       - Note: This task MODIFIES existing functions `remove_processed_cluster_errors()` and `append_error_to_queue()` in `error_queue.py` (they currently don't have locking)
       - Required imports: `import fcntl`, `import os`, `import time`
       - Update `remove_processed_cluster_errors()` function to use file locking (Unix/macOS/Linux only):
         1. Open queue file for reading: `with open(queue_file, "r", encoding="utf-8") as f:`
         2. Acquire shared lock: `fcntl.flock(f.fileno(), fcntl.LOCK_SH)` (shared lock for reading)
         3. Read all errors: `errors = [json.loads(line) for line in f if line.strip()]`
         4. Release lock: `fcntl.flock(f.fileno(), fcntl.LOCK_UN)`
         5. Close file (automatic with context manager)
         6. Filter errors (remove by index): `remaining_errors = [error for idx, error in enumerate(errors) if idx not in processed_cluster_indices]`
         7. Open queue file for writing: `with open(queue_file, "w", encoding="utf-8") as f:`
         8. Acquire exclusive lock: `fcntl.flock(f.fileno(), fcntl.LOCK_EX)` (exclusive lock for writing)
         9. Write remaining errors: `for error in remaining_errors: f.write(json.dumps(error) + "\n")`
         10. Flush and sync: `f.flush()`, `os.fsync(f.fileno())`
         11. Release lock: `fcntl.flock(f.fileno(), fcntl.LOCK_UN)`
         12. Close file (automatic with context manager)
       - Update `append_error_to_queue()` function to use file locking (Unix/macOS/Linux only):
         1. Open queue file for appending: `with open(queue_file, "a", encoding="utf-8") as f:`
         2. Acquire exclusive lock: `fcntl.flock(f.fileno(), fcntl.LOCK_EX)`
         3. Append error: `f.write(json.dumps(error) + "\n")`
         4. Flush and sync: `f.flush()`, `os.fsync(f.fileno())`
         5. Release lock: `fcntl.flock(f.fileno(), fcntl.LOCK_UN)`
         6. Close file (automatic with context manager)
       - Error handling: If lock acquisition fails, retry up to 3 times with 0.1s delay (`time.sleep(0.1)`), then raise IOError
       - Test: Verify queue operations work correctly with locking, no race conditions

   10.2 **Add module reloading after successful validation** [S]
       - File: `coding_agent/agent.py`
       - Note: This is called from VALIDATE node (Task 9.1) after queue cleanup, before creating final_output
       - Call `coding_agent.reloader.reload_parser(self._parsers_dir)` to reload all cluster modules
       - Note: `reload_parser()` returns a TimeParser instance, but the return value can be ignored in the workflow
       - Note: The caller (notebook or application code) should create a new TimeParser instance after workflow completes to use the updated modules
       - This reloads all cluster modules using `importlib.reload()` and refreshes the main parser module
       - Update state: `node_output["parser_reloaded"] = True` (done in VALIDATE node)
       - Test: Verify modules are reloaded and parser works with new modules

### 11.0 Create LLM Output Parsing Utilities [M]

   11.1 **Create llm_output_parsing_util.py** [M]
       - File: `utils/llm_output_parsing_util.py`
       - Required imports: `import logging` (for warnings), `from typing import Any`
       - Add helper functions to extract structured data from parsed JSON responses (from `llm_json_parser.parse_llm_json_extraction_response()`)
       - Functions should extract data from `list[dict[str, Any]] | None` returned by JSON parser
       - Add function: `extract_reason_node_output(parsed_response: list[dict[str, Any]] | None) -> dict[str, Any]`
         - Function signature: `def extract_reason_node_output(parsed_response: list[dict[str, Any]] | None) -> dict[str, Any]:`
         - Input: `parsed_response` from `parse_llm_json_extraction_response()` - list of dicts or None
         - Expected structure: First dict in list should contain keys: `clusters` (list), `selected_clusters` (list), `total_errors_analyzed` (int), `total_clusters_identified` (int), `clusters_selected_count` (int)
         - Error handling: 
           - If `parsed_response` is None or empty list, raise ValueError with message "REASON node response is None or empty"
           - If first dict is missing required key `clusters`, raise ValueError with message "REASON node response missing 'clusters' key"
           - If first dict is missing required key `selected_clusters`, raise ValueError with message "REASON node response missing 'selected_clusters' key"
           - If optional keys are missing (`total_errors_analyzed`, etc.), log warning and use default values (0 for ints, empty list for lists)
         - Returns dict with keys: 
           - `clusters` (list[dict]): List of cluster dicts, each with keys: cluster_id, error_indices, commonality, examples, suggested_approach, parsability, error_count
           - `selected_clusters` (list[str]): List of cluster_id strings selected for processing
           - `total_errors_analyzed` (int): Total number of errors analyzed (default 0 if missing)
           - `total_clusters_identified` (int): Total number of clusters found (default 0 if missing)
           - `clusters_selected_count` (int): Number of clusters selected (default len(selected_clusters) if missing)
       - Add function: `extract_plan_node_output(parsed_response: list[dict[str, Any]] | None) -> dict[str, Any]`
         - Function signature: `def extract_plan_node_output(parsed_response: list[dict[str, Any]] | None) -> dict[str, Any]:`
         - Input: `parsed_response` from `parse_llm_json_extraction_response()` - list of dicts or None
         - Expected structure: First dict in list should contain key: `cluster_plans` (list of plan dicts)
         - Error handling:
           - If `parsed_response` is None or empty list, raise ValueError with message "PLAN node response is None or empty"
           - If first dict is missing required key `cluster_plans`, raise ValueError with message "PLAN node response missing 'cluster_plans' key"
         - Returns dict with key: 
           - `cluster_plans` (list[dict]): List of plan dicts, each with keys: cluster_id, parsing_strategy, code_structure, test_cases (list), dependencies (list), edge_cases (list)
       - Add function: `extract_act_node_output(parsed_response: list[dict[str, Any]] | None) -> dict[str, Any]`
         - Function signature: `def extract_act_node_output(parsed_response: list[dict[str, Any]] | None) -> dict[str, Any]:`
         - Input: `parsed_response` from `parse_llm_json_extraction_response()` - list of dicts or None
         - Expected structure: First dict in list should contain keys: `cluster_modules` (dict), `test_files` (dict)
         - Error handling:
           - If `parsed_response` is None or empty list, raise ValueError with message "ACT node response is None or empty"
           - If first dict is missing required key `cluster_modules`, raise ValueError with message "ACT node response missing 'cluster_modules' key"
           - If first dict is missing required key `test_files`, raise ValueError with message "ACT node response missing 'test_files' key"
         - Returns dict with keys:
           - `cluster_modules` (dict[str, str]): Dictionary mapping cluster_id (str) to module code (str)
           - `test_files` (dict[str, str]): Dictionary mapping cluster_id (str) to test file code (str)
       - Error handling strategy: Always raise ValueError for missing required keys (don't return partial data) - let calling code handle errors based on fail_fast setting
       - Note: Pattern similar to examples in `DO_NOT_COMMIT/ai_aided_annotations/util.py` but adapted for our use case
       - Test: Verify parsing functions extract data correctly from various JSON response formats, verify error handling for None/empty/missing keys

### 12.0 Create Test Infrastructure [M]

   12.1 **Create test utilities** [S]
       - File: `time_parser/tests/test_utils.py`
       - Add function: `assert_valid_datetime(result: datetime | None, input_text: str) -> None`
       - Assertions: result is not None, is datetime instance, has timezone, is UTC
       - Test: Verify utility function works correctly

   12.2 **Create shared pytest fixtures** [S]
       - File: `time_parser/tests/conftest.py`
       - Add fixture: `parser()` that returns TimeParser instance
       - Test: Verify fixture works in pytest

### 13.0 Create Demo Jupyter Notebook [M]

   13.1 **Create notebook structure** [M]
       - File: `notebooks/demo.ipynb`
       - Follow style of reference_notebook.ipynb (path setup, imports, logging configuration)
       - Add cells for: imports, path setup, logging, Gemini LLM initialization
       - Use GOOGLE_API_KEY from environment
       - Initialize Gemini using langchain_google_genai.ChatGoogleGenerativeAI
       - Model: "gemini-3" (or "gemini-2.5-flash" as fallback)
       - Temperature: 0.2, max_tokens: 8192
       - Test: Verify notebook runs without errors

   13.1a **Add problem demonstration with fixture data** [S]
       - File: `notebooks/demo.ipynb`
       - Add cell: Load fixture file using pandas: `pd.read_json('tests/fixtures/follow_up_tasks_202512121435.jsonl', lines=True)`
       - Add cell: Display sample rows showing the problem:
         - Show a few rows where `deadline_at` is null (parsing failed)
         - Show a few rows where `deadline_at` has a value (parsing succeeded)
       - Use pandas DataFrame display to show: `customer_id`, `timing_description`, `deadline_at` columns
       - Add explanatory text: "This demonstrates the problem - some timing descriptions fail to parse (deadline_at is null), while others succeed"
       - Test: Verify fixture file loads correctly and display shows both error and success cases

   13.2 **Add initial parser state demonstration** [S]
       - File: `notebooks/demo.ipynb`
       - Cell: Show initial TimeParser implementation
       - Cell: Show empty parsers/ directory
       - Cell: Test parser with basic inputs ("asap", "now")
       - Test: Verify cells execute correctly

   13.3 **Add error collection demonstration** [M]
       - File: `notebooks/demo.ipynb`
       - Cell: Wrap parser with exception interceptor
       - Cell: Test parser with various inputs that will fail
       - Cell: Display error queue contents
       - Cell: Show clustering preview (what REASON node will do)
       - Test: Verify errors are collected and displayed

   13.4 **Add agent activation demonstration** [M]
       - File: `notebooks/demo.ipynb`
       - Cell: Check error count threshold (use `coding_agent.error_queue.get_error_count()`)
       - Cell: Build NodePrompts using `build_node_prompts()` function:
         ```python
         from coding_agent.prompts import build_node_prompts
         node_prompts = build_node_prompts()
         # node_prompts contains template user prompts (not formatted)
         # User prompts will be formatted in each node method with actual data
         ```
       - Cell: Build NodeLLMs: `NodeLLMs(reason=llm, plan=llm, act=llm, validate=llm)`
       - Cell: Generate thread_id: `thread_id = f"coding_agent_{uuid.uuid4().hex[:8]}"` or use timestamp-based ID
       - Required imports: `import uuid` (for thread_id generation)
       - Cell: Initialize CodingAgentWorkflow:
         ```python
         from coding_agent.agent import CodingAgentWorkflow
         from coding_agent.base import DEFAULT_RATE_LIMITING_CONFIG
         
         workflow = CodingAgentWorkflow(
             node_llms=node_llms,
             node_prompts=node_prompts,  # Template prompts, not formatted
             thread_id=thread_id,
             error_queue_path="error_queue.jsonl",
             parsers_dir="time_parser/parsers",
             tests_dir="time_parser/tests",
             rate_limiting_config=DEFAULT_RATE_LIMITING_CONFIG,
             fail_fast=False,
             error_logging=True,
             debug_logging=False,
             enforce_structured_llm_output=False,  # Must be False for schema=None
         )
         ```
       - Note: error_queue_path, parsers_dir, tests_dir are instance variables (not in initial state)
       - Cell: Create initial state: `initial_state = AnnotationState(messages=[], node_output=None, final_output=None)`
       - Cell: Run agent workflow: `result = workflow.run(initial_state=initial_state)`
       - Cell: Display agent results from result dict (keys: success, processed_clusters, errors_removed_count, parser_updated, tests_passed, retry_count, generated_cluster_modules, generated_test_files, cluster_error_indices)
       - Note: `workflow.run()` returns `final_state["final_output"] or {}` - see design doc Section 4.2 for exact structure
       - Test: Verify agent runs successfully

   13.5 **Add success verification demonstration** [S]
       - File: `notebooks/demo.ipynb`
       - Cell: Reload parser after agent update
       - Cell: Test parser with previously failing inputs
       - Cell: Run pytest and display results
       - Cell: Show updated error queue (should have fewer errors)
       - Test: Verify complete self-healing flow works

### 14.0 Integration Testing and Refinement [M]

   14.1 **Test complete end-to-end flow** [M]
       - Create test error queue with sample errors from `tests/fixtures/follow_up_tasks_202512121435.jsonl`
       - Filter fixture data: Select rows where `deadline_at` is null (these are the parsing failures)
       - Copy selected rows to error_queue.jsonl for testing
       - Run agent workflow
       - Verify: Errors are clustered correctly
       - Verify: Code is generated and written correctly
       - Verify: Tests are generated and pass
       - Verify: Parser is updated and works with new modules
       - Verify: Processed errors are removed from queue
       - Test: Complete flow works without errors

   14.2 **Refine prompts based on actual LLM behavior** [M]
       - Run agent with real errors
       - Observe LLM responses and adjust prompts if needed
       - Ensure JSON output format is consistent
       - Ensure code generation quality is acceptable
       - Test: Prompts produce reliable, high-quality outputs

   14.3 **Add error handling and logging** [S]
       - Add comprehensive error handling throughout workflow
       - Add logging for debugging (use Python logging module)
       - Handle LLM API errors gracefully
       - Handle file I/O errors gracefully
       - Test: Error handling works correctly in various failure scenarios

## Manual Testing Guide

### Prerequisites

- All tasks 1.0-14.0 must be completed
- GOOGLE_API_KEY environment variable set
- Project dependencies installed (`pip install -e .`)
- Jupyter notebook environment available
- Test error data available (from `tests/fixtures/follow_up_tasks_202512121435.jsonl`)

### Test 1: Basic Parser Functionality

**Steps:**

1. **Start Jupyter notebook:**
   ```bash
   cd agi_house_gemini_3_hackathon_12132025
   jupyter notebook notebooks/demo.ipynb
   ```

2. **Run initial setup cells:**
   - Verify path setup works
   - Verify imports succeed
   - Verify Gemini LLM initializes correctly

3. **Test basic parser:**
   - Create TimeParser instance
   - Test with "asap" - should succeed
   - Test with "now" - should succeed
   - Test with "tomorrow" - should fail (not yet implemented)
   - Expected: Basic cases work, unknown patterns raise ValueError

### Test 2: Error Collection

**Steps:**

1. **Wrap parser with exception interceptor:**
   - Use `intercept_parser_errors()` decorator
   - Test with various inputs that will fail

2. **Verify error logging:**
   - Check error_queue.jsonl file
   - Verify errors are logged in correct format
   - Verify timing_description field contains failed input text

3. **Test error queue utilities:**
   - Use `coding_agent.error_queue.read_error_queue()` to read errors
   - Verify error count matches expected

### Test 3: Agent Workflow - REASON Node

**Steps:**

1. **Prepare error queue:**
   - Populate error_queue.jsonl with sample errors (at least ERROR_THRESHOLD errors)
   - Use errors from `tests/fixtures/follow_up_tasks_202512121435.jsonl`
   - Filter for rows where `deadline_at` is null (these represent parsing failures)

2. **Initialize agent:**
   - Create CodingAgentWorkflow instance
   - Configure with Gemini LLM and prompts
   - Set error_queue_path, parsers_dir, tests_dir

3. **Run REASON node:**
   - Execute workflow up to REASON node
   - Verify: Errors are read from queue
   - Verify: LLM clusters errors correctly
   - Verify: Selected clusters are chosen (up to CLUSTER_BATCH_SIZE)
   - Verify: State contains error_clusters and selected_clusters

### Test 4: Agent Workflow - PLAN Node

**Steps:**

1. **Continue from Test 3:**
   - Run PLAN node with cluster analysis from REASON node

2. **Verify planning:**
   - Verify: LLM creates code plans for each selected cluster
   - Verify: Plans include parsing_strategy, code_structure, test_cases, dependencies
   - Verify: State contains code_plan

### Test 5: Agent Workflow - ACT Node

**Steps:**

1. **Continue from Test 4:**
   - Run ACT node with code plan from PLAN node

2. **Verify code generation:**
   - Verify: Cluster module files are written to parsers/ directory
   - Verify: Test files are written to tests/ directory
   - Verify: Files are syntactically correct Python
   - Verify: Module files export parse() function
   - Verify: Test files use pytest with parameterized tests

3. **Verify file locking:**
   - Test: Files are written atomically
   - Test: No race conditions during file writing

### Test 6: Agent Workflow - VALIDATE Node

**Steps:**

1. **Continue from Test 5:**
   - Run VALIDATE node after ACT node

2. **Verify test execution:**
   - Verify: pytest runs on tests/ directory
   - Verify: Test results are parsed correctly
   - Verify: all_passed flag is set correctly
   - Verify: retry_count is incremented

3. **Test retry logic:**
   - If tests fail, verify workflow loops back to PLAN node
   - Verify retry_count increments correctly
   - After MAX_RETRY_ATTEMPTS, verify failed batch is logged to failed_batches.jsonl

### Test 7: Complete End-to-End Flow

**Steps:**

1. **Start with empty parsers/ directory:**
   - Clear any existing cluster modules

2. **Populate error queue:**
   - Load fixture data from `tests/fixtures/follow_up_tasks_202512121435.jsonl`
   - Filter for rows where `deadline_at` is null (parsing failures)
   - Add at least ERROR_THRESHOLD errors to error_queue.jsonl
   - Use diverse error patterns (relative dates, specific dates, time ranges)

3. **Run complete workflow:**
   - Initialize and run CodingAgentWorkflow
   - Let workflow complete (REASON → PLAN → ACT → VALIDATE → success)

4. **Verify success:**
   - Verify: Cluster modules are created in parsers/
   - Verify: Test files are created in tests/
   - Verify: All tests pass
   - Verify: Processed errors are removed from error_queue.jsonl
   - Verify: Parser can be reloaded and works with new modules

5. **Test parser with new capabilities:**
   - Reload parser using `coding_agent.reloader.reload_parser()`
   - Test parser with inputs that previously failed
   - Verify: Parser now handles these inputs correctly

### Test 8: Safety Valve (Failed Batch Handling)

**Steps:**

1. **Create scenario where tests will fail:**
   - Manually corrupt a generated test file or module file
   - Or use prompts that generate invalid code

2. **Run workflow:**
   - Let workflow retry up to MAX_RETRY_ATTEMPTS

3. **Verify safety valve:**
   - Verify: After max retries, failed batch is logged to failed_batches.jsonl
   - Verify: Workflow ends with "failure" status (does not block)
   - Verify: Error queue still contains original errors (not removed)
   - Verify: Agent can continue with next batch

### Troubleshooting

**Issue: LLM returns invalid JSON**
- Check: Prompt templates are correctly formatted
- Check: LLM response parsing handles markdown code blocks
- Solution: Improve JSON extraction in `_parse_json_response()`

**Issue: Generated code has syntax errors**
- Check: ACT node prompt requires complete, syntactically correct code
- Check: LLM is using correct temperature (0.1-0.3)
- Solution: Refine ACT node prompt, add syntax validation before writing files

**Issue: Tests fail after code generation**
- Check: Test assertions match actual parser behavior
- Check: Expected datetime values are correct (relative to "now")
- Solution: Refine PLAN node to create better test expectations, or adjust ACT node to generate better code

**Issue: Module reloading doesn't work**
- Check: importlib.reload() is called correctly
- Check: Module paths are correct
- Solution: Verify reloader.py implementation, check sys.modules state

**Issue: Error queue cleanup removes wrong errors**
- Check: Error indices mapping is correct (cluster indices to global indices)
- Solution: Verify index mapping logic in queue cleanup

## Future Work

### Parallel Processing Opportunities

Currently, the workflow processes clusters sequentially. Future enhancements could include:

1. **Parallel Cluster Processing:**
   - After REASON node, process multiple clusters in parallel
   - Each cluster gets its own PLAN/ACT/VALIDATE chain
   - Merge results before final queue cleanup

2. **Parallel Test Execution:**
   - Run tests for different clusters in parallel
   - Use pytest-xdist for parallel test execution

3. **Parallel Code Generation:**
   - Generate parser modules and test files in parallel
   - Each cluster is independent, so parallel generation is safe

### Audit Trail for Parser Changes

Currently, parser modules are overwritten without backup. Future enhancement:

1. **Version Control Integration:**
   - Keep history of parser module versions
   - Track which errors triggered which changes
   - Enable rollback to previous versions
   - Similar to audit trail patterns in existing codebase

### Enhanced Error Handling

1. **Better Error Recovery:**
   - Detect and handle specific error types (syntax errors, import errors, etc.)
   - Provide more specific feedback to LLM for retry attempts

2. **Confidence Scoring:**
   - Add confidence scores to generated code
   - Only deploy high-confidence changes automatically
   - Require human review for low-confidence changes

### Performance Monitoring

1. **Metrics Collection:**
   - Track parsing success rate over time
   - Monitor agent performance (clustering accuracy, code generation quality)
   - Track test pass rates and retry frequencies

2. **Performance Optimization:**
   - Cache LLM responses for similar error patterns
   - Optimize module loading and reloading
   - Reduce redundant code generation

### Production Deployment Considerations

Before production deployment, address:

1. **Security:**
   - Validate generated code before execution
   - Sandbox code execution if possible
   - Review generated code for security issues

2. **Reliability:**
   - Add comprehensive error handling
   - Implement circuit breakers for LLM API calls
   - Add monitoring and alerting

3. **Scalability:**
   - Handle large error queues efficiently
   - Implement batching and pagination for error processing
   - Optimize for high-throughput scenarios

