# Development Plan Sprint 1: Self-Healing Time Parser - Foundation and LangGraph Integration

## Overview

This sprint implements a self-healing time parser system that uses an LLM-based coding agent to automatically update parsing logic when encountering new time expression patterns. The system learns from failures and improves itself without manual intervention. This sprint establishes the foundation (MVP) and integrates LangGraph workflow with REASON/PLAN/ACT/VALIDATE pattern. The implementation uses Google Gemini 3 via LangChain, implements modular parser architecture with cluster-specific modules, and includes comprehensive test-driven validation.

## Critical Directives

1. **Confirm before marking tasks** - Never mark a task as complete with checkmark (✓) without explicit user permission.
2. **Ask permission to proceed** - After marking a task complete, state the next task and request permission before continuing.
3. **Never proceed without permission** - Always wait for explicit approval before starting any task.
4. **Follow specification exactly** - Adhere strictly to this design specification and referenced design documents.
5. **Follow coding standards** - Obey all Python coding standards (PEP 8, type hints, docstrings).
6. **Pure ASCII markdown** - Keep all sprint documentation in ASCII-only markdown format.
7. **Test after each step** - Run tests and manual verification for affected components after implementing each functional chunk.
8. **Preserve existing functionality** - Ensure no regressions in existing codebase utilities.
9. **Modular architecture** - Use modular architecture with one module per error cluster (not single large file).
10. **UTC timezone requirement** - All datetime objects must be timezone-aware with UTC timezone.

## Sprint Goals

- Install project dependencies via pyproject.toml.
- Create TimeParser class with dynamic cluster module loading.
- Implement exception interceptor/wrapper for error logging.
- Create error queue management (JSONL format with append/read/remove functionality).
- Implement prompt templates for REASON, PLAN, and ACT nodes.
- Create CodingAgentWorkflow class extending WorkflowBase with LangGraph R/P/A pattern.
- Implement REASON node for error clustering using LLM.
- Implement PLAN node for code planning using LLM.
- Implement ACT node for code generation using LLM.
- Implement VALIDATE node for pytest test execution.
- Add retry logic with conditional edges and safety valve (failed batch logging).
- Create demo Jupyter notebook with complete self-healing flow demonstration.
- Test complete end-to-end flow (error collection → clustering → code generation → validation → parser update).

## Implementation Clarifications

### Module Paths and Locations

1. **Time Parser Module:**
   - Main parser: `time_parser/parser.py` (orchestrator, rarely updated)
   - Cluster modules: `time_parser/parsers/<cluster_id>.py` (generated by agent, one per cluster)
   - Exception wrapper: `time_parser/wrapper.py` (error interceptor)

2. **Test Directory:**
   - Test files: `time_parser/tests/test_<cluster_id>.py` (one per cluster module)
   - Shared utilities: `time_parser/tests/test_utils.py`
   - Shared fixtures: `time_parser/tests/conftest.py`

3. **Coding Agent:**
   - Main workflow: `coding_agent/agent.py` (CodingAgentWorkflow class)
   - Configuration: `coding_agent/config.py` (constants: CLUSTER_BATCH_SIZE, MAX_RETRY_ATTEMPTS, ERROR_THRESHOLD)
   - Queue management: `coding_agent/error_queue.py` (read, append, remove functions)
   - Test runner: `coding_agent/test_runner.py` (pytest integration)
   - Module reloader: `coding_agent/reloader.py` (dynamic module reloading)
   - Prompt templates: `coding_agent/prompts.py` (add prompt template strings)

4. **Utilities:**
   - LLM helpers: `utils/llm_helpers.py` (call_llm_with_prompt function - already exists)
   - Dynamic loading: `utils/dynamic_loading.py` (module loading utilities - already exists)
   - JSON parser: `utils/llm_json_parser.py` (LLMJsonParser class for parsing LLM JSON responses - already exists)
   - JSON helpers: `utils/json_helpers.py` (is_valid_json function - already exists)
   - LLM output parsing utilities: `utils/llm_output_parsing_util.py` (helper functions to extract structured data from parsed JSON responses - to be created)

5. **Demo Notebook:**
   - File: `notebooks/demo.ipynb` (Jupyter notebook for demonstration)

6. **Runtime Files:**
   - Error queue: `error_queue.jsonl` (repository root, gitignored)
   - Failed batches: `failed_batches.jsonl` (repository root, gitignored)

7. **Test Fixtures:**
   - Error examples: `tests/fixtures/follow_up_tasks_202512121435.jsonl` (source data for testing and demonstration)

### Architectural Decisions

1. **Modular Parser Architecture:**
   - Main parser (`time_parser/parser.py`) orchestrates by discovering and loading cluster-specific modules
   - Each error cluster gets its own module file in `time_parser/parsers/`
   - Each module exports a `parse(text: str) -> datetime | None` function
   - Main parser tries each cluster module in sequence until one succeeds
   - No limit on number of clusters - scales naturally with new module files

2. **LLM Integration:**
   - Use Google Gemini 3 via LangChain (`langchain_google_genai.ChatGoogleGenerativeAI`)
   - API key from `GOOGLE_API_KEY` environment variable
   - Use `call_llm_with_prompt()` from `utils/llm_helpers.py` with `schema=None` (schema-less output)
   - Parse JSON responses manually from LLM text output
   - Temperature: 0.1-0.3 for code generation
   - Model: "gemini-3" (or "gemini-2.5-flash" as fallback)

3. **LangGraph Workflow Pattern:**
   - Use `WorkflowBase` from `coding_agent/base.py` as base class
   - Use `AnnotationState` for workflow state management (no custom state class needed)
   - Custom data stored in `state["node_output"]` as a dict (e.g., `state["node_output"]["error_clusters"]`)
   - Nodes: REASON → PLAN → ACT → VALIDATE
   - Conditional edge from VALIDATE: success → END, retry → PLAN, failure → END (safety valve)
   - Node methods signature: `def _node_<name>(self, state: AnnotationState) -> AnnotationState:`
   - Access state: `state["node_output"]` (dict), `state["messages"]`, `state["final_output"]`
   - Update state: Modify `state["node_output"]` dict, call `self._update_state(state, messages=[...], node_output=node_output)` or set directly, return state
   - JSON parsing: Use `self._llm_json_parser.parse_llm_json_extraction_response()` (available from WorkflowBase), returns `list[dict[str, Any]] | None`
   - JSON extraction: Use helper functions from `utils.llm_output_parsing_util` to extract structured data from parsed responses
   - Prompts: Access via `self._node_prompts.<node_name>` (returns SystemAndUserPromptPair with .system_prompt and .user_prompt)
   - Prompt formatting: System prompts are constants, user prompts are templates formatted in each node method using `.format()`
   - LLM calls: Use `self._call_llm_with_prompt(node_name="...", system_prompt=..., user_prompt=..., schema=None)` - always use `schema=None`
   - Initial state: `AnnotationState(messages=[], node_output=None, final_output=None)` - pass to `workflow.run(initial_state=...)`
   - Thread ID: Generate unique ID (e.g., `f"coding_agent_{uuid.uuid4().hex[:8]}"` or timestamp-based) - required for WorkflowBase.__init__()

4. **Error Queue Format:**
   - JSONL format (one JSON object per line)
   - Fields: `timing_description` (primary), `auxiliary_pretty` (optional), `customer_id`, `deadline_at`
   - Easy to append, read batch, and remove processed items

5. **Error Clustering:**
   - LLM-based semantic clustering in REASON node
   - Process up to CLUSTER_BATCH_SIZE clusters per batch (default: 3, configurable)
   - Prioritize parsable clusters over context-dependent or ambiguous ones
   - Cluster IDs become module filenames (lowercase_with_underscores)

6. **Code Generation:**
   - ACT node generates complete module files (not snippets)
   - One module file per cluster: `time_parser/parsers/<cluster_id>.py`
   - One test file per cluster: `time_parser/tests/test_<cluster_id>.py`
   - All code must be syntactically correct and ready to execute
   - Store generated modules and test files as dictionaries with cluster_id (without `.py` suffix) as keys

7. **Test Validation:**
   - VALIDATE node runs pytest on `time_parser/tests/` directory
   - All tests must pass before proceeding to module reload
   - If tests fail, loop back to PLAN node (up to MAX_RETRY_ATTEMPTS)
   - After max retries, log failed batch to `failed_batches.jsonl` and give up

8. **Module Reloading:**
   - After successful validation, reload cluster modules using `importlib.reload()`
   - Call `TimeParser.reload_cluster_modules()` to refresh parser registry
   - Use file locking (fcntl) to prevent race conditions during updates

9. **Exception Interceptor:**
   - Decorator/context manager pattern that wraps parser calls
   - Catches exceptions, logs to error queue, re-raises or returns None
   - Does not block original code execution

10. **Safety Valve:**
    - Failed batches (after max retries) logged to `failed_batches.jsonl`
    - Agent continues with next batch (does not block processing)
    - Failed batches can be reviewed manually later

## Sprint Tasks

### 1.0 Install Dependencies and Verify Environment [S]

   1.1 **Install project dependencies** [S]
       - Navigate to project root: `cd agi_house_gemini_3_hackathon_12132025`
       - Run: `pip install -e .` (installs main dependencies including pandas)
       - Optional: Run `pip install -e ".[dev]"` to also install dev dependencies (jupyter, matplotlib, tqdm)
       - Verify all dependencies install correctly (langchain, langchain-google-genai, langgraph, pydantic, pytest, python-dateutil, pandas)
       - Note: This installs from pyproject.toml

   1.2 **Verify GOOGLE_API_KEY environment variable** [S]
       - Check: `GOOGLE_API_KEY` is set in environment
       - Test: Create simple test script to verify Gemini API access
       - Note: API key should be available before proceeding with LLM integration tasks

### 2.0 Create TimeParser Class with Dynamic Module Loading [M]

   2.1 **Create TimeParser class** [M]
       - File: `time_parser/parser.py`
       - Implement `__init__()`: Initialize `_cluster_parsers` dict and call `_load_cluster_modules()`
       - Implement `_load_cluster_modules()`: Discover all `.py` files in `parsers/` directory, import each module, register `parse()` function
       - Implement `reload_cluster_modules()`: Reload existing modules using `importlib.reload()`, then call `_load_cluster_modules()`
       - Implement `parse(text: str) -> datetime`: Try each cluster parser in order, return first successful result or raise ValueError
       - Handle errors gracefully: If a cluster module fails to load, continue with other modules
       - Test: Verify parser loads modules correctly, handles missing modules gracefully

   2.2 **Create initial basic parser for testing** [S]
       - Initially, parser should handle basic cases: "asap", "now" (for testing)
       - These can be hardcoded in `parse()` method initially
       - Agent will generate cluster modules later
       - Test: Verify basic parsing works before agent integration

### 3.0 Create Exception Interceptor/Wrapper [M]

   3.1 **Create intercept_parser_errors decorator** [M]
       - File: `time_parser/wrapper.py`
       - Implement decorator pattern: `intercept_parser_errors(parser_instance)`
       - Behavior: Catch exceptions from parser, log to error queue (JSONL format), re-raise exception
       - Error entry format: `{"customer_id": None, "deadline_at": None, "timing_description": text, "auxiliary_pretty": json.dumps({...})}`
       - Use `coding_agent.error_queue.append_error_to_queue()` for logging
       - Test: Verify errors are logged correctly to error_queue.jsonl

   3.2 **Test exception interceptor with TimeParser** [S]
       - Create test script that uses wrapped parser
       - Test with inputs that fail parsing
       - Verify errors appear in error_queue.jsonl
       - Verify original code execution continues (does not block)

### 4.0 Implement Prompt Templates [M]

   4.1 **Add REASON node prompt templates** [M]
       - File: `coding_agent/prompts.py`
       - Add: `REASON_NODE_SYSTEM_PROMPT_TEMPLATE` (from Section 18.1 of design doc) - constant string
       - Add: `REASON_NODE_USER_PROMPT_TEMPLATE` (from Section 18.1 of design doc) - template with `{error_queue_contents}` placeholder
       - Note: System prompt is constant, user prompt is formatted in REASON node with actual error queue contents
       - Note: `{error_queue_contents}` should contain entire content of parsing errors (only rows where parsing failed, i.e., `deadline_at` is null)
       - Test: Verify templates are valid Python strings, can be formatted

   4.2 **Add PLAN node prompt templates** [M]
       - File: `coding_agent/prompts.py`
       - Add: `PLAN_NODE_SYSTEM_PROMPT_TEMPLATE` (from Section 18.2 of design doc) - constant string
       - Add: `PLAN_NODE_USER_PROMPT_TEMPLATE` (from Section 18.2 of design doc) - template with `{cluster_analysis}` and `{existing_cluster_modules}` placeholders
       - Note: System prompt is constant, user prompt is formatted in PLAN node with cluster analysis and existing modules
       - Note: `{cluster_analysis}` is the output from REASON node, `{existing_cluster_modules}` are modules already in `time_parser/parsers/` directory (for context)
       - Test: Verify templates are valid Python strings, can be formatted

   4.3 **Add ACT node prompt templates** [M]
       - File: `coding_agent/prompts.py`
       - Add: `ACT_NODE_SYSTEM_PROMPT_TEMPLATE` (from Section 18.3 of design doc) - constant string
       - Add: `ACT_NODE_USER_PROMPT_TEMPLATE` (from Section 18.3 of design doc) - template with `{code_plan}` placeholder
       - Note: System prompt is constant, user prompt is formatted in ACT node with code plan
       - Note: `{code_plan}` is the output from PLAN node
       - Test: Verify templates are valid Python strings, can be formatted

   4.4 **Create helper function to build NodePrompts** [S]
       - File: `coding_agent/prompts.py`
       - Add function: `build_node_prompts() -> NodePrompts`
       - Function creates SystemAndUserPromptPair instances using the constant system prompts and template user prompts
       - Returns NodePrompts with reason, plan, act prompts populated (user prompts are templates, not formatted yet)
       - Note: This function is called at workflow initialization to build NodePrompts
       - Note: User prompts are formatted in each node method (not at initialization) - mirror pattern from `DO_NOT_COMMIT/ai_aided_annotations/tasks`
       - In nodes, access prompts via `self._node_prompts.<node_name>` (returns SystemAndUserPromptPair), then format user prompt with `.format()`
       - Test: Verify function creates valid NodePrompts object

### 5.0 Create CodingAgentWorkflow Class Structure [M]

   5.1 **Create CodingAgentWorkflow class skeleton** [M]
       - File: `coding_agent/agent.py`
       - Extend `WorkflowBase` from `coding_agent.base`
       - Implement `__init__()`: Accept parameters:
         - `error_queue_path: str | Path` - Path to error queue JSONL file
         - `parsers_dir: str | Path` - Directory for cluster parser modules
         - `tests_dir: str | Path` - Directory for test files
         - `node_llms: NodeLLMs` - LLM configuration for each node
         - `node_prompts: NodePrompts` - Prompt templates for each node
         - `thread_id: str` - Unique thread identifier
         - `rate_limiting_config: RateLimitingConfig = DEFAULT_RATE_LIMITING_CONFIG` - Rate limiting config (from coding_agent.base)
         - `fail_fast: bool = False` - Whether to fail fast on errors
         - `error_logging: bool = True` - Whether to log errors
         - `debug_logging: bool = False` - Whether to enable debug logging
         - `enforce_structured_llm_output: bool = False` - Whether to enforce structured output (should be False for schema=None)
       - Store configuration as instance variables: `self._error_queue_path`, `self._parsers_dir`, `self._tests_dir`
       - Call `super().__init__()` with: `node_llms`, `node_prompts`, `thread_id`, `enforce_structured_llm_output`, `rate_limiting_config`, `fail_fast`, `error_logging`, `debug_logging`
       - Import required dependencies: WorkflowBase, AnnotationState, NodeLLMs, NodePrompts, RateLimitingConfig, DEFAULT_RATE_LIMITING_CONFIG, etc.
       - Note: No custom state class needed - use `AnnotationState` and store custom data in `state["node_output"]` dict
       - Note: error_queue_path, parsers_dir, tests_dir are instance variables (not stored in state)
       - Test: Verify class can be instantiated

   5.2 **Implement _add_workflow_nodes_and_edges method** [M]
       - File: `coding_agent/agent.py`
       - Add nodes: `workflow.add_node("reason", self._reason_node)`, etc.
       - Set entry point: `workflow.add_edge(START, "reason")`
       - Add sequential edges: `workflow.add_edge("reason", "plan")`, `workflow.add_edge("plan", "act")`, `workflow.add_edge("act", "validate")`
       - Add conditional edge from validate: `workflow.add_conditional_edges("validate", self._should_retry, {"success": END, "retry": "plan", "failure": END})`
       - Note: `_should_retry()` method should return "success", "retry", or "failure" based on test results and retry count
       - Test: Verify workflow graph structure is correct

### 6.0 Implement REASON Node [M]

   6.1 **Implement _reason_node method** [M]
       - File: `coding_agent/agent.py`
       - Method signature: `def _reason_node(self, state: AnnotationState) -> AnnotationState:`
       - Read full error queue using `coding_agent.error_queue.read_error_queue(self._error_queue_path)` - store as `all_errors`
       - Filter errors: Only include rows where `deadline_at` is null (parsing failures - these are the errors we want to fix) - store as `filtered_errors`
       - Create mapping from filtered index to global index: `filtered_to_global = {filtered_idx: global_idx for global_idx, error in enumerate(all_errors) if error.get("deadline_at") is None}`
       - Error handling: If no errors found, log warning and return state with empty clusters
       - Format error queue contents as JSONL string: `error_queue_contents = "\n".join(json.dumps(error) for error in filtered_errors)`
       - Note: Each line in error_queue_contents should be a complete JSON object with `timing_description` field (the text that failed to parse)
       - Get prompts: `system_and_user_prompt_pair = self._node_prompts.reason`
       - Extract: `system_prompt = system_and_user_prompt_pair.system_prompt` (constant, no formatting)
       - Format user prompt in node: `user_prompt = system_and_user_prompt_pair.user_prompt.format(error_queue_contents=error_queue_contents)`
       - Call LLM: `response = self._call_llm_with_prompt(node_name="reason", system_prompt=system_prompt, user_prompt=user_prompt, schema=None)`
       - Error handling: If LLM call fails, log error and raise exception (or handle based on fail_fast setting)
       - Parse JSON: Use `self._llm_json_parser.parse_llm_json_extraction_response(response.content, fail_fast=self._fail_fast, context_identifier=("Node", "reason"), debug_logging=self._debug_logging)`
       - Returns: `list[dict[str, Any]] | None`
       - Error handling: If parsed_response is None or empty, log error and raise ValueError (or handle based on fail_fast)
       - Extract data: Use helper function from `utils.llm_output_parsing_util.extract_reason_node_output(parsed_response)` to extract clusters and selected_clusters
       - Error handling: If helper function returns None or missing required keys, log error and raise ValueError
       - Map error indices from filtered to global: For each cluster in extracted data, convert error_indices (which are relative to filtered_errors) to global indices using `filtered_to_global` mapping
       - Store in state: `node_output = state["node_output"] or {}`, set `node_output["error_clusters"]` (list of cluster dicts with cluster_id, error_indices [now global], commonality, examples, etc.) and `node_output["selected_clusters"]` (list of cluster_ids)
       - Also store `node_output["cluster_error_indices"]` as dict mapping cluster_id to list of global error indices: `{cluster["cluster_id"]: [filtered_to_global[idx] for idx in cluster["error_indices"]] for cluster in extracted_data["clusters"] if cluster["cluster_id"] in extracted_data["selected_clusters"]}`
       - Construct messages for state update: `from langchain_core.messages import SystemMessage, HumanMessage; system_message = SystemMessage(content=system_prompt); human_message = HumanMessage(content=user_prompt); messages = [system_message, human_message, response]`
       - Call `self._update_state(state, messages=messages, node_output=node_output)` and return updated state
       - Test: Verify REASON node clusters errors correctly with mock LLM response

### 7.0 Implement PLAN Node [M]

   7.1 **Implement _plan_node method** [M]
       - File: `coding_agent/agent.py`
       - Method signature: `def _plan_node(self, state: AnnotationState) -> AnnotationState:`
       - Read existing cluster modules from parsers_dir: List `.py` files (excluding `__init__.py`), read each file, extract brief description
       - Format existing_cluster_modules: Create list of dicts with structure: `[{"module_name": "relative_dates", "description": "Brief description extracted from module docstring or first few lines"}]`
       - Format as string: `existing_modules_str = json.dumps(existing_cluster_modules, indent=2)`
       - Note: Module names should be unambiguous without `.py` suffix (e.g., `relative_dates` not `relative_dates.py`)
       - Format cluster_analysis from state: Get full cluster data from `state["node_output"]["error_clusters"]`, filter by `state["node_output"]["selected_clusters"]` (list of cluster_ids)
       - Create cluster_analysis dict: For each selected cluster_id, get the full cluster dict from error_clusters (includes cluster_id, error_indices, commonality, examples, suggested_approach, parsability, error_count)
       - Format as string: `cluster_analysis = json.dumps([cluster for cluster in error_clusters if cluster["cluster_id"] in selected_clusters], indent=2)`
       - Error handling: If selected_clusters is empty or None, log warning and return state with empty code_plan
       - Get prompts: `system_and_user_prompt_pair = self._node_prompts.plan`
       - Extract: `system_prompt = system_and_user_prompt_pair.system_prompt` (constant, no formatting)
       - Format user prompt in node: `user_prompt = system_and_user_prompt_pair.user_prompt.format(cluster_analysis=cluster_analysis, existing_cluster_modules=existing_modules_str)`
       - Call LLM: `response = self._call_llm_with_prompt(node_name="plan", system_prompt=system_prompt, user_prompt=user_prompt, schema=None)`
       - Error handling: If LLM call fails, log error and raise exception (or handle based on fail_fast setting)
       - Parse JSON: Use `self._llm_json_parser.parse_llm_json_extraction_response(...)` as in REASON node
       - Error handling: If parsed_response is None or empty, log error and raise ValueError
       - Extract code_plan: Use helper function from `utils.llm_output_parsing_util.extract_plan_node_output(parsed_response)` to extract code_plan dict (should have cluster_plans array)
       - Error handling: If helper function returns None or missing cluster_plans key, log error and raise ValueError
       - Update state: `node_output = state["node_output"] or {}`, set `node_output["code_plan"] = code_plan`
       - Construct messages for state update: `from langchain_core.messages import SystemMessage, HumanMessage; system_message = SystemMessage(content=system_prompt); human_message = HumanMessage(content=user_prompt); messages = [system_message, human_message, response]`
       - Call `self._update_state(state, messages=messages, node_output=node_output)` and return updated state
       - Test: Verify PLAN node creates code plans correctly with mock LLM response

### 8.0 Implement ACT Node [M]

   8.1 **Implement _act_node method** [M]
       - File: `coding_agent/agent.py`
       - Method signature: `def _act_node(self, state: AnnotationState) -> AnnotationState:`
       - Get code_plan from state: `code_plan = state["node_output"]["code_plan"]` (output from PLAN node)
       - Error handling: If code_plan is None or missing, log error and raise ValueError
       - Format code_plan as JSON string: `code_plan_str = json.dumps(code_plan, indent=2)`
       - Get prompts: `system_and_user_prompt_pair = self._node_prompts.act`
       - Extract: `system_prompt = system_and_user_prompt_pair.system_prompt` (constant, no formatting)
       - Format user prompt in node: `user_prompt = system_and_user_prompt_pair.user_prompt.format(code_plan=code_plan_str)`
       - Call LLM: `response = self._call_llm_with_prompt(node_name="act", system_prompt=system_prompt, user_prompt=user_prompt, schema=None)`
       - Error handling: If LLM call fails, log error and raise exception (or handle based on fail_fast setting)
       - Parse JSON: Use `self._llm_json_parser.parse_llm_json_extraction_response(...)` as in REASON node
       - Error handling: If parsed_response is None or empty, log error and raise ValueError
       - Extract cluster_modules and test_files: Use helper function from `utils.llm_output_parsing_util.extract_act_node_output(parsed_response)` to extract dictionaries from parsed response
       - Error handling: If helper function returns None or missing cluster_modules/test_files keys, log error and raise ValueError
       - Store as dictionaries with cluster names (without `.py` suffix) as keys: `cluster_modules[cluster_id] = code_string`, `test_files[cluster_id] = test_code_string`
       - Write cluster module files: For each cluster_id in cluster_modules, write to `Path(self._parsers_dir) / f"{cluster_id}.py"`
       - Write test files: For each cluster_id in test_files, write to `Path(self._tests_dir) / f"test_{cluster_id}.py"`
       - Use `_write_file_with_lock()` helper for atomic file writing
       - Error handling: If file writing fails, log error and raise IOError (or handle based on fail_fast)
       - Update state: `node_output = state["node_output"] or {}`, set `node_output["generated_cluster_modules"] = cluster_modules` and `node_output["generated_test_files"] = test_files` (both as dicts with cluster_id keys)
       - Cluster error indices already stored in REASON node: `node_output["cluster_error_indices"]` (dict mapping cluster_id to list of global error indices)
       - Construct messages for state update: `from langchain_core.messages import SystemMessage, HumanMessage; system_message = SystemMessage(content=system_prompt); human_message = HumanMessage(content=user_prompt); messages = [system_message, human_message, response]`
       - Call `self._update_state(state, messages=messages, node_output=node_output)` and return updated state
       - Test: Verify ACT node generates and writes files correctly with mock LLM response

   8.2 **Add file writing helper with locking** [S]
       - File: `coding_agent/agent.py`
       - Add function: `_write_file_with_lock(file_path: Path, content: str) -> None`
       - Use fcntl.flock() for file locking (Unix) or msvcrt for Windows
       - Write to temporary file first, then atomically rename
       - Handle locking errors gracefully
       - Test: Verify file writing works correctly with locking

### 9.0 Implement VALIDATE Node [M]

   9.1 **Implement _validate_node method** [M]
       - File: `coding_agent/agent.py`
       - Method signature: `def _validate_node(self, state: AnnotationState) -> AnnotationState:`
       - Use `coding_agent.test_runner.run_pytest(self._tests_dir)` to run tests
       - Note: `run_pytest()` returns dict with keys: `all_passed` (bool), `test_output` (str), `test_errors` (str), `returncode` (int)
       - Error handling: If pytest execution fails (exception), log error and set test_results with all_passed=False
       - Parse test results: `test_results = run_pytest_result` (already in correct format)
       - Get current retry_count: `retry_count = state["node_output"].get("retry_count", 0)`
       - Increment retry_count: `retry_count += 1`
       - Update state: `node_output = state["node_output"] or {}`, set `node_output["test_results"] = test_results` and `node_output["retry_count"] = retry_count`
       - If tests pass (test_results["all_passed"] is True):
         - Perform queue cleanup first (see Task 10.1): Collect error indices from `node_output["cluster_error_indices"]`, flatten, remove from queue, calculate `errors_removed_count`
         - Perform module reloading (see Task 10.2): Call `coding_agent.reloader.reload_parser(self._parsers_dir)` (return value can be ignored - caller will create new TimeParser instance)
         - Get processed_clusters: `processed_clusters = node_output.get("selected_clusters", [])` (all selected clusters are processed if tests pass)
         - Calculate errors_removed_count: `errors_removed_count = sum(len(indices) for indices in node_output.get("cluster_error_indices", {}).values())`
         - Create final_output dict with structure matching design doc Section 4.2:
           - `final_output = {"success": True, "processed_clusters": processed_clusters, "errors_removed_count": errors_removed_count, "parser_updated": True, "tests_passed": True, "retry_count": retry_count, "generated_cluster_modules": node_output.get("generated_cluster_modules", {}), "generated_test_files": node_output.get("generated_test_files", {}), "cluster_error_indices": node_output.get("cluster_error_indices", {})}`
         - Set `state["final_output"] = final_output`
         - Update `node_output["errors_removed_count"] = errors_removed_count` and `node_output["parser_reloaded"] = True`
       - Update state and return (no messages needed for VALIDATE node - it doesn't call LLM)
       - Test: Verify VALIDATE node runs pytest and parses results correctly

   9.2 **Implement _should_retry method** [S]
       - File: `coding_agent/agent.py`
       - Method signature: `def _should_retry(self, state: AnnotationState) -> str:`
       - Get test_results: `test_results = state["node_output"].get("test_results", {})`
       - Check test_results["all_passed"]: If True, return "success"
       - Get retry_count: `retry_count = state["node_output"].get("retry_count", 0)`
       - Check retry_count against MAX_RETRY_ATTEMPTS from `coding_agent.config`
       - If retry_count < MAX_RETRY_ATTEMPTS, return "retry"
       - Otherwise, call `self._log_failed_batch(state)` and return "failure"
       - Test: Verify retry logic works correctly for all scenarios

   9.3 **Implement _log_failed_batch method** [S]
       - File: `coding_agent/agent.py`
       - Method signature: `def _log_failed_batch(self, state: AnnotationState) -> None:`
       - Get data from state: `selected_clusters = state["node_output"].get("selected_clusters", [])`, `test_results = state["node_output"].get("test_results", {})`, `retry_count = state["node_output"].get("retry_count", 0)`, `cluster_error_indices = state["node_output"].get("cluster_error_indices", {})`
       - Get original errors: Read error queue and extract errors for failed clusters using cluster_error_indices
       - Create failed_batch_entry dict with structure:
         - `{"selected_clusters": list of cluster_ids, "cluster_error_indices": cluster_error_indices dict, "error_count": total error count, "test_results": test_results dict, "retry_count": retry_count, "timestamp": ISO timestamp string, "error_samples": list of sample error dicts (first 3 errors from each cluster)}`
       - Append to failed_batches.jsonl using JSONL format: `coding_agent.error_queue.append_error_to_queue("failed_batches.jsonl", failed_batch_entry)`
       - Error handling: If file write fails, log warning but don't raise exception (non-critical)
       - Test: Verify failed batches are logged correctly

### 10.0 Implement Queue Cleanup and Module Reloading [M]

   10.1 **Add queue cleanup after successful validation** [M]
       - File: `coding_agent/agent.py`
       - Note: This is called from VALIDATE node (Task 9.1) after tests pass, before creating final_output
       - Collect error indices from `state["node_output"]["cluster_error_indices"]`
       - This dict maps cluster_id to list of error indices (global indices from original queue, mapped in REASON node)
       - Flatten all error indices from all clusters into a single list: `all_error_indices = [idx for indices in cluster_error_indices.values() for idx in indices]`
       - Use `coding_agent.error_queue.remove_processed_cluster_errors(self._error_queue_path, all_error_indices)` to remove errors
       - Calculate errors_removed_count: `errors_removed_count = len(all_error_indices)`
       - Note: Error indices are tracked through the workflow - REASON node maps filtered indices to global indices and stores in `cluster_error_indices` dict
       - Test: Verify processed errors are removed from queue correctly

   10.2 **Add module reloading after successful validation** [S]
       - File: `coding_agent/agent.py`
       - Note: This is called from VALIDATE node (Task 9.1) after queue cleanup, before creating final_output
       - Call `coding_agent.reloader.reload_parser(self._parsers_dir)` to reload all cluster modules
       - Note: `reload_parser()` returns a TimeParser instance, but the return value can be ignored in the workflow
       - Note: The caller (notebook or application code) should create a new TimeParser instance after workflow completes to use the updated modules
       - This reloads all cluster modules using `importlib.reload()` and refreshes the main parser module
       - Update state: `node_output["parser_reloaded"] = True` (done in VALIDATE node)
       - Test: Verify modules are reloaded and parser works with new modules

### 11.0 Create LLM Output Parsing Utilities [M]

   11.1 **Create llm_output_parsing_util.py** [M]
       - File: `utils/llm_output_parsing_util.py`
       - Add helper functions to extract structured data from parsed JSON responses (from `llm_json_parser.parse_llm_json_extraction_response()`)
       - Functions should extract data from `list[dict[str, Any]] | None` returned by JSON parser
       - Add function: `extract_reason_node_output(parsed_response: list[dict[str, Any]] | None) -> dict[str, Any]`
         - Extracts clusters, selected_clusters, and other fields from REASON node response
         - Error handling: If parsed_response is None or empty, return empty dict with default values or raise ValueError (document behavior)
         - Error handling: If first dict in list is missing required keys (clusters, selected_clusters), log warning and return partial data or raise ValueError
         - Returns dict with keys: `clusters` (list of cluster dicts), `selected_clusters` (list of cluster_ids), `total_errors_analyzed` (int), `total_clusters_identified` (int), `clusters_selected_count` (int)
       - Add function: `extract_plan_node_output(parsed_response: list[dict[str, Any]] | None) -> dict[str, Any]`
         - Extracts code_plan with cluster_plans array from PLAN node response
         - Error handling: If parsed_response is None or empty, return empty dict or raise ValueError
         - Error handling: If missing cluster_plans key, log warning and return empty dict or raise ValueError
         - Returns dict with key: `cluster_plans` (list of plan dicts, each with cluster_id, parsing_strategy, code_structure, test_cases, dependencies, edge_cases)
       - Add function: `extract_act_node_output(parsed_response: list[dict[str, Any]] | None) -> dict[str, Any]`
         - Extracts cluster_modules and test_files dictionaries from ACT node response
         - Error handling: If parsed_response is None or empty, return empty dict or raise ValueError
         - Error handling: If missing cluster_modules or test_files keys, log warning and return partial data or raise ValueError
         - Returns dict with keys: `cluster_modules` (dict mapping cluster_id to code string), `test_files` (dict mapping cluster_id to test code string)
       - Note: Pattern similar to examples in `DO_NOT_COMMIT/ai_aided_annotations/util.py` but adapted for our use case
       - Error handling strategy: Log warnings for missing optional keys, raise ValueError for missing required keys (or return empty dicts based on fail_fast parameter if added)
       - Test: Verify parsing functions extract data correctly from various JSON response formats

### 12.0 Create Test Infrastructure [M]

   12.1 **Create test utilities** [S]
       - File: `time_parser/tests/test_utils.py`
       - Add function: `assert_valid_datetime(result: datetime | None, input_text: str) -> None`
       - Assertions: result is not None, is datetime instance, has timezone, is UTC
       - Test: Verify utility function works correctly

   12.2 **Create shared pytest fixtures** [S]
       - File: `time_parser/tests/conftest.py`
       - Add fixture: `parser()` that returns TimeParser instance
       - Test: Verify fixture works in pytest

### 13.0 Create Demo Jupyter Notebook [M]

   13.1 **Create notebook structure** [M]
       - File: `notebooks/demo.ipynb`
       - Follow style of reference_notebook.ipynb (path setup, imports, logging configuration)
       - Add cells for: imports, path setup, logging, Gemini LLM initialization
       - Use GOOGLE_API_KEY from environment
       - Initialize Gemini using langchain_google_genai.ChatGoogleGenerativeAI
       - Model: "gemini-3" (or "gemini-2.5-flash" as fallback)
       - Temperature: 0.2, max_tokens: 8192
       - Test: Verify notebook runs without errors

   13.1a **Add problem demonstration with fixture data** [S]
       - File: `notebooks/demo.ipynb`
       - Add cell: Load fixture file using pandas: `pd.read_json('tests/fixtures/follow_up_tasks_202512121435.jsonl', lines=True)`
       - Add cell: Display sample rows showing the problem:
         - Show a few rows where `deadline_at` is null (parsing failed)
         - Show a few rows where `deadline_at` has a value (parsing succeeded)
       - Use pandas DataFrame display to show: `customer_id`, `timing_description`, `deadline_at` columns
       - Add explanatory text: "This demonstrates the problem - some timing descriptions fail to parse (deadline_at is null), while others succeed"
       - Test: Verify fixture file loads correctly and display shows both error and success cases

   13.2 **Add initial parser state demonstration** [S]
       - File: `notebooks/demo.ipynb`
       - Cell: Show initial TimeParser implementation
       - Cell: Show empty parsers/ directory
       - Cell: Test parser with basic inputs ("asap", "now")
       - Test: Verify cells execute correctly

   13.3 **Add error collection demonstration** [M]
       - File: `notebooks/demo.ipynb`
       - Cell: Wrap parser with exception interceptor
       - Cell: Test parser with various inputs that will fail
       - Cell: Display error queue contents
       - Cell: Show clustering preview (what REASON node will do)
       - Test: Verify errors are collected and displayed

   13.4 **Add agent activation demonstration** [M]
       - File: `notebooks/demo.ipynb`
       - Cell: Check error count threshold (use `coding_agent.error_queue.get_error_count()`)
       - Cell: Build NodePrompts using `build_node_prompts()` function (returns NodePrompts with template user prompts, not formatted)
       - Cell: Build NodeLLMs: `NodeLLMs(reason=llm, plan=llm, act=llm, validate=llm)`
       - Cell: Generate thread_id: `thread_id = f"coding_agent_{uuid.uuid4().hex[:8]}"` or use timestamp-based ID
       - Cell: Initialize CodingAgentWorkflow: `workflow = CodingAgentWorkflow(node_llms=..., node_prompts=..., thread_id=..., error_queue_path=..., parsers_dir=..., tests_dir=..., rate_limiting_config=..., fail_fast=..., error_logging=..., debug_logging=...)`
       - Note: error_queue_path, parsers_dir, tests_dir are instance variables (not in initial state)
       - Cell: Create initial state: `initial_state = AnnotationState(messages=[], node_output=None, final_output=None)`
       - Cell: Run agent workflow: `result = workflow.run(initial_state=initial_state)`
       - Cell: Display agent results from result dict (keys: success, processed_clusters, errors_removed_count, parser_updated, tests_passed, retry_count, generated_cluster_modules, generated_test_files, cluster_error_indices)
       - Note: `workflow.run()` returns `final_state["final_output"] or {}` - see design doc Section 4.2 for exact structure
       - Test: Verify agent runs successfully

   13.5 **Add success verification demonstration** [S]
       - File: `notebooks/demo.ipynb`
       - Cell: Reload parser after agent update
       - Cell: Test parser with previously failing inputs
       - Cell: Run pytest and display results
       - Cell: Show updated error queue (should have fewer errors)
       - Test: Verify complete self-healing flow works

### 14.0 Integration Testing and Refinement [M]

   14.1 **Test complete end-to-end flow** [M]
       - Create test error queue with sample errors from `tests/fixtures/follow_up_tasks_202512121435.jsonl`
       - Filter fixture data: Select rows where `deadline_at` is null (these are the parsing failures)
       - Copy selected rows to error_queue.jsonl for testing
       - Run agent workflow
       - Verify: Errors are clustered correctly
       - Verify: Code is generated and written correctly
       - Verify: Tests are generated and pass
       - Verify: Parser is updated and works with new modules
       - Verify: Processed errors are removed from queue
       - Test: Complete flow works without errors

   14.2 **Refine prompts based on actual LLM behavior** [M]
       - Run agent with real errors
       - Observe LLM responses and adjust prompts if needed
       - Ensure JSON output format is consistent
       - Ensure code generation quality is acceptable
       - Test: Prompts produce reliable, high-quality outputs

   14.3 **Add error handling and logging** [S]
       - Add comprehensive error handling throughout workflow
       - Add logging for debugging (use Python logging module)
       - Handle LLM API errors gracefully
       - Handle file I/O errors gracefully
       - Test: Error handling works correctly in various failure scenarios

## Manual Testing Guide

### Prerequisites

- All tasks 1.0-14.0 must be completed
- GOOGLE_API_KEY environment variable set
- Project dependencies installed (`pip install -e .`)
- Jupyter notebook environment available
- Test error data available (from `tests/fixtures/follow_up_tasks_202512121435.jsonl`)

### Test 1: Basic Parser Functionality

**Steps:**

1. **Start Jupyter notebook:**
   ```bash
   cd agi_house_gemini_3_hackathon_12132025
   jupyter notebook notebooks/demo.ipynb
   ```

2. **Run initial setup cells:**
   - Verify path setup works
   - Verify imports succeed
   - Verify Gemini LLM initializes correctly

3. **Test basic parser:**
   - Create TimeParser instance
   - Test with "asap" - should succeed
   - Test with "now" - should succeed
   - Test with "tomorrow" - should fail (not yet implemented)
   - Expected: Basic cases work, unknown patterns raise ValueError

### Test 2: Error Collection

**Steps:**

1. **Wrap parser with exception interceptor:**
   - Use `intercept_parser_errors()` decorator
   - Test with various inputs that will fail

2. **Verify error logging:**
   - Check error_queue.jsonl file
   - Verify errors are logged in correct format
   - Verify timing_description field contains failed input text

3. **Test error queue utilities:**
   - Use `coding_agent.error_queue.read_error_queue()` to read errors
   - Verify error count matches expected

### Test 3: Agent Workflow - REASON Node

**Steps:**

1. **Prepare error queue:**
   - Populate error_queue.jsonl with sample errors (at least ERROR_THRESHOLD errors)
   - Use errors from `tests/fixtures/follow_up_tasks_202512121435.jsonl`
   - Filter for rows where `deadline_at` is null (these represent parsing failures)

2. **Initialize agent:**
   - Create CodingAgentWorkflow instance
   - Configure with Gemini LLM and prompts
   - Set error_queue_path, parsers_dir, tests_dir

3. **Run REASON node:**
   - Execute workflow up to REASON node
   - Verify: Errors are read from queue
   - Verify: LLM clusters errors correctly
   - Verify: Selected clusters are chosen (up to CLUSTER_BATCH_SIZE)
   - Verify: State contains error_clusters and selected_clusters

### Test 4: Agent Workflow - PLAN Node

**Steps:**

1. **Continue from Test 3:**
   - Run PLAN node with cluster analysis from REASON node

2. **Verify planning:**
   - Verify: LLM creates code plans for each selected cluster
   - Verify: Plans include parsing_strategy, code_structure, test_cases, dependencies
   - Verify: State contains code_plan

### Test 5: Agent Workflow - ACT Node

**Steps:**

1. **Continue from Test 4:**
   - Run ACT node with code plan from PLAN node

2. **Verify code generation:**
   - Verify: Cluster module files are written to parsers/ directory
   - Verify: Test files are written to tests/ directory
   - Verify: Files are syntactically correct Python
   - Verify: Module files export parse() function
   - Verify: Test files use pytest with parameterized tests

3. **Verify file locking:**
   - Test: Files are written atomically
   - Test: No race conditions during file writing

### Test 6: Agent Workflow - VALIDATE Node

**Steps:**

1. **Continue from Test 5:**
   - Run VALIDATE node after ACT node

2. **Verify test execution:**
   - Verify: pytest runs on tests/ directory
   - Verify: Test results are parsed correctly
   - Verify: all_passed flag is set correctly
   - Verify: retry_count is incremented

3. **Test retry logic:**
   - If tests fail, verify workflow loops back to PLAN node
   - Verify retry_count increments correctly
   - After MAX_RETRY_ATTEMPTS, verify failed batch is logged to failed_batches.jsonl

### Test 7: Complete End-to-End Flow

**Steps:**

1. **Start with empty parsers/ directory:**
   - Clear any existing cluster modules

2. **Populate error queue:**
   - Load fixture data from `tests/fixtures/follow_up_tasks_202512121435.jsonl`
   - Filter for rows where `deadline_at` is null (parsing failures)
   - Add at least ERROR_THRESHOLD errors to error_queue.jsonl
   - Use diverse error patterns (relative dates, specific dates, time ranges)

3. **Run complete workflow:**
   - Initialize and run CodingAgentWorkflow
   - Let workflow complete (REASON → PLAN → ACT → VALIDATE → success)

4. **Verify success:**
   - Verify: Cluster modules are created in parsers/
   - Verify: Test files are created in tests/
   - Verify: All tests pass
   - Verify: Processed errors are removed from error_queue.jsonl
   - Verify: Parser can be reloaded and works with new modules

5. **Test parser with new capabilities:**
   - Reload parser using `coding_agent.reloader.reload_parser()`
   - Test parser with inputs that previously failed
   - Verify: Parser now handles these inputs correctly

### Test 8: Safety Valve (Failed Batch Handling)

**Steps:**

1. **Create scenario where tests will fail:**
   - Manually corrupt a generated test file or module file
   - Or use prompts that generate invalid code

2. **Run workflow:**
   - Let workflow retry up to MAX_RETRY_ATTEMPTS

3. **Verify safety valve:**
   - Verify: After max retries, failed batch is logged to failed_batches.jsonl
   - Verify: Workflow ends with "failure" status (does not block)
   - Verify: Error queue still contains original errors (not removed)
   - Verify: Agent can continue with next batch

### Troubleshooting

**Issue: LLM returns invalid JSON**
- Check: Prompt templates are correctly formatted
- Check: LLM response parsing handles markdown code blocks
- Solution: Improve JSON extraction in `_parse_json_response()`

**Issue: Generated code has syntax errors**
- Check: ACT node prompt requires complete, syntactically correct code
- Check: LLM is using correct temperature (0.1-0.3)
- Solution: Refine ACT node prompt, add syntax validation before writing files

**Issue: Tests fail after code generation**
- Check: Test assertions match actual parser behavior
- Check: Expected datetime values are correct (relative to "now")
- Solution: Refine PLAN node to create better test expectations, or adjust ACT node to generate better code

**Issue: Module reloading doesn't work**
- Check: importlib.reload() is called correctly
- Check: Module paths are correct
- Solution: Verify reloader.py implementation, check sys.modules state

**Issue: Error queue cleanup removes wrong errors**
- Check: Error indices mapping is correct (cluster indices to global indices)
- Solution: Verify index mapping logic in queue cleanup

## Future Work

### Parallel Processing Opportunities

Currently, the workflow processes clusters sequentially. Future enhancements could include:

1. **Parallel Cluster Processing:**
   - After REASON node, process multiple clusters in parallel
   - Each cluster gets its own PLAN/ACT/VALIDATE chain
   - Merge results before final queue cleanup

2. **Parallel Test Execution:**
   - Run tests for different clusters in parallel
   - Use pytest-xdist for parallel test execution

3. **Parallel Code Generation:**
   - Generate parser modules and test files in parallel
   - Each cluster is independent, so parallel generation is safe

### Audit Trail for Parser Changes

Currently, parser modules are overwritten without backup. Future enhancement:

1. **Version Control Integration:**
   - Keep history of parser module versions
   - Track which errors triggered which changes
   - Enable rollback to previous versions
   - Similar to audit trail patterns in existing codebase

### Enhanced Error Handling

1. **Better Error Recovery:**
   - Detect and handle specific error types (syntax errors, import errors, etc.)
   - Provide more specific feedback to LLM for retry attempts

2. **Confidence Scoring:**
   - Add confidence scores to generated code
   - Only deploy high-confidence changes automatically
   - Require human review for low-confidence changes

### Performance Monitoring

1. **Metrics Collection:**
   - Track parsing success rate over time
   - Monitor agent performance (clustering accuracy, code generation quality)
   - Track test pass rates and retry frequencies

2. **Performance Optimization:**
   - Cache LLM responses for similar error patterns
   - Optimize module loading and reloading
   - Reduce redundant code generation

### Production Deployment Considerations

Before production deployment, address:

1. **Security:**
   - Validate generated code before execution
   - Sandbox code execution if possible
   - Review generated code for security issues

2. **Reliability:**
   - Add comprehensive error handling
   - Implement circuit breakers for LLM API calls
   - Add monitoring and alerting

3. **Scalability:**
   - Handle large error queues efficiently
   - Implement batching and pagination for error processing
   - Optimize for high-throughput scenarios

